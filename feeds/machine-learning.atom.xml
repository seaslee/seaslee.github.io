<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>AI's bazaar</title><link href="/" rel="alternate"></link><link href="/feeds/machine-learning.atom.xml" rel="self"></link><id>/</id><updated>2014-05-02T17:00:00+08:00</updated><entry><title>线性回归模型</title><link href="/posts/2014/05/linear_model/" rel="alternate"></link><updated>2014-05-02T17:00:00+08:00</updated><author><name>Webdancer</name></author><id>tag:,2014-05-02:posts/2014/05/linear_model/</id><summary type="html">&lt;p&gt;本章讨论机器学习里面最简单，最基本的linear regression model（线性回归模型）。首先学习线性回归模型，然后在学习使用概率一章中讲的参数求解的方式进行参数求解，最后是最优化时候使用的导数法和随机梯度下降的方法。&lt;/p&gt;
&lt;h3&gt;线性回归(linear regression)&lt;/h3&gt;
&lt;p&gt;一般来说，机器学习的目的就是找一个函数$f:\mathcal{X} \rightarrow \mathcal{Y}$。在回归问题中，$\mathcal{Y}$是连续的，一般属于$\mathbb{R}$。比如我们要根据某个人烟龄来预测他的能活得岁数；根据过去几天的温度，预测未来几天的温度等。我们关心模型具体的输出值，这些值都是有具体的意义的。&lt;/p&gt;
&lt;p&gt;线性回归模型是一种结构简单的统计模型，$\mathcal{X}$与$\mathcal{Y}$是线性关系。虽然简单，了解线性模型的原理对于我们理解后面的一些概念非常重要。线性回归是什么呢？简单来说，回归就是由一堆数来预测出一堆我们感兴趣的数。就如上面说的，回归模型的输出是连续的数值，这些数值通常就是我们需要的，比如预测大学的排名；这区别于
以后的我们要学习的分类问题，分类模型的输出时离散的，通常分类的离散数值只是一些符号代表，没有实际的意义。下面从最简单的单变量线性回归模型说起，再拓展到多变量的模
型。&lt;/p&gt;
&lt;h4&gt;单变量线性回归模型&lt;/h4&gt;
&lt;p&gt;单变量线性模型用到的一些符号：已知含有$N$个特征变量$x$以及对应的目标变量$t$的数据集合，记作：$\{(x^{(1)},t^{(1)}),(x^{(2)
},t^{(2)}),\dots.,(x^{(N)},t^{(N)})\}$，其中第$i$
个变量写为$(x^{(i)},t^{(i)})$，这里为了与下面多变量进行区别，用上标来表示第几个实例。新的$x$的预测值为$\hat{t}$。顾名思义，线性回归也就是说$t$与$x$之间有线性关系，即目标值是输入值的线性组合。单变量线性回归的模型假设(hypothesis) 为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\hat{t}(x)=\theta_{0} + \theta_{1} x
\end{equation}&lt;/p&gt;
&lt;p&gt;其中，$x$为模型输入变量，$\hat{t}$为模型预测的目标变量值的近似，$ \theta=(\theta_{0},\theta_{1})^T$为模型参数&lt;/p&gt;
&lt;p&gt;下面我们的目标就是如何求解模型中的参数来确定模型。直观上衡量模型好坏就是输出变量$\hat t$与目标变量$t$相差的程度，两者越接近，则越能反映$t$与$x$之间的关系，所以可以使用平方损失函数(squared loss function）作为求解模型的参数的优化目标函数，如下：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathbb E=\frac{1}{2}\sum_{n=1}^{N}\{t^{(n)}-\hat{t}_\theta(x)\}^2
\label{ls_error}
\end{equation}&lt;/p&gt;
&lt;p&gt;根据公式的数学意义我们可以看到，$\mathbb E$值越小，假设越好，这种方法就是很常见的&lt;strong&gt;最小二乘法&lt;/strong&gt; 。下面的工作就是最小化$\mathbb
E$来得到最优的参数值$ \theta^*$。
现在我们对线性模型有了一个比较直观的印象，知道了什么是线性模型，得到一个优化的目标函数求解参数。这也是机器学习的一般过程：提出模型，根据假设得到一个最优化问题，
求解最优化参数，最后评估模型。下面看一个简单的例子：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt; 
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt;  &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;linear_model&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;lm&lt;/span&gt;
&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c"&gt;#generate data&lt;/span&gt;
&lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt; &lt;span class="c"&gt;#data size&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; 
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;savez&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;reg.npz&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;var&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;reg.npz&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="c"&gt;#predict the data&lt;/span&gt;
&lt;span class="n"&gt;clf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LinearRegression&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="c"&gt;# a wrap of scipy.linalg.lstsq&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vstack&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; 
&lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y_pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c"&gt;#plot &lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;.&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;--&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;r&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;.&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;k&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;g&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="/images/c_1_0.png" /&gt;&lt;/p&gt;
&lt;h4&gt;多变量线性回归模型&lt;/h4&gt;
&lt;p&gt;下面来看一下多变量的线性回归模型：已知含有$N$个观测变量$\mathbf x$，以及对应的目标变量$t$的数据集合，记作：$\{(\mathbf
x^{(1)},t^{(1)}),(\mathbf x^{(2)},t^{(2)}),\dots.,(\mathbf
x^{(N)},t^{(N)})\}$,其中第$i$个变量写为$(\mathbf x^{(i)},t^{(i)})$，新的$\mathbf x$
的预测值$\hat{t}$。 多变量线性回归的模型假设为：&lt;/p&gt;
&lt;p&gt;\begin{eqnarray}
\hat{t}&amp;amp;=&amp;amp; \theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+\dots+\theta_{D}*x_{D}\
\nonumber &amp;amp;=&amp;amp;  {\theta^{T}}\mathbf x
\label{mullr}
\end{eqnarray}&lt;/p&gt;
&lt;p&gt;其中，$\mathbf x=(x_0,x_1,x_2,\dots,x_D)^{T}$为模型输入向量，$\hat{t}$为模型输出向量，
$ \theta=(\theta_{0},\theta_{1},\dots,\theta_D)$为参数，$D$为特征向量的维数。可以看到模型是关于输入$\mathbf x$ 和参数$
\theta$ 的线性函数。这样把$N$个观测变量$\mathbf x$的数据集收集在一个矩阵中$ \mathbf X$中，记作：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\label{xmat}
    \mathbf X  =
\left(
  \begin{array}{cccc}
  \mathbf x_1^1 &amp;amp; \mathbf x_2^1 &amp;amp; \dots &amp;amp; \mathbf x_D^1 \\
\mathbf x_1^2 &amp;amp; \mathbf x_2^2 &amp;amp; \dots &amp;amp; \mathbf x_D^2 \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
\mathbf x_1^N &amp;amp; \mathbf x_2^N &amp;amp; \dots &amp;amp; \mathbf x_D^N
  \end{array}
\right)
 =
 \left(
  \begin{array}{cccc}
1 &amp;amp; \mathbf x_1^1 &amp;amp; \dots &amp;amp; \mathbf x_D^1 \\
1 &amp;amp; \mathbf x_1^2 &amp;amp; \dots &amp;amp; \mathbf x_D^2\\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
1 &amp;amp; \mathbf x_1^N &amp;amp; \dots &amp;amp; \mathbf x_D^N
  \end{array}
\right)
\end{equation}&lt;/p&gt;
&lt;p&gt;矩阵中每一列代表一维的特征，每一行代表一个实例，行数代表了实例数，列数代表特征维数。当然也不一定使用这种方式来表示数据集，也完全可以用该矩阵的转置来表示，不过这
种表示是一种常用的表示，Matlab中就是用这种方式来表示的，所以使用这种表示可以更容易的使用matlab中的库函数求解一些统计数字，比如方差，均值等。&lt;/p&gt;
&lt;h4&gt;多变量线性基函数回归模型&lt;/h4&gt;
&lt;p&gt;对模型引入“基函数”，这样模型仅是相对于参数的线性模型，而不是相对于输入的线性模型，从而使模型增强。原来$t$与$\mathbf
x$是只能是线性关系，引入基函数后，$t$与$\mathbf x$是非线性的情况也可以做处理了。模型假设如下：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\hat{t}= \theta_{0}+\theta_{1}\phi_{1}(\mathbf
x)+\dots+\theta_{M}\phi_{M}(\mathbf x)
\label{mulvar_lbm}
\end{equation}&lt;/p&gt;
&lt;p&gt;我们可以看到，通过引入“基函数”，参数数目由$D$变成了$M$，基函数的作用就是使输入空间变到了另外一个空间。
假设用向量表示形式为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\hat{t}= \theta^{T} \phi(\mathbf x)
 \label{mulvar_lbm_vec}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中，$ \theta=(\theta_0,\theta_1,\dots,\theta_M)$，$ \phi=(1,\phi_1,\dots,\phi_M)$。
在理解“基函数”的作用的过程中，可能会出现问题。这里我们看一下基函数的作用：
 由前面基函数的定义，它是将整个变量$\mathbf x$，一个向量转化为一个标量(在wikipedia上的模型与我们这里的模型有些不同)。在实际的模式识别应用
中，我们会对原始的数据进行一些预处理或是特征提取，原始的数据可以看做$\mathbf x$，而处理后的数据看做$\phi(\mathbf x)$。
 实际处理问题时怎么对原始数据进行处理时非常灵活的。基函数的选取对于我们模型的讨论没有影响，所以后面我们将 $\phi(\mathbf x)=\mathbf
x$来进行讨论。&lt;/p&gt;
&lt;p&gt;下面要关注的就是如何得到最优的参数$ \theta^*$，下面就用最大似然估计方法来求解模型中的参数。&lt;/p&gt;
&lt;h4&gt;最大似然估计&lt;/h4&gt;
&lt;p&gt;在第二章中，对最大似然估计的原理已经做了介绍。下面看一下用最大似然估计来估计多变量线性回归模型中的参数。回归问题是一个典型的监督学习问题，使用最大似然估计，首先要确定的是$p(t|\mathbf x, \theta)$的条件分布。假设目标变量与模型预测变量之间存在噪声，两者关系如下：&lt;/p&gt;
&lt;p&gt;\begin{equation}
t=\hat{t}+\epsilon
\label{tarvar}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中，$\epsilon$为噪声值，假设其为高斯噪声，符合高斯分布$\mathcal
N(\epsilon|0,\beta^{-1})$。那么$t$条件分布的密度函数为$p(t|\mathbf x, \theta)$：&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(t|\mathbf x, \theta,\beta)=\mathcal N(t|\hat{t},\beta^{-1})
\label{target}
\end{equation}&lt;/p&gt;
&lt;p&gt;已知训练集的特征变量集合为$\mathbf X=\{\mathbf x^1,\mathbf x^2,\dots,\mathbf
x^N\}$,对应的目标值为：$\mathbf
t=(t^1,t^2,\dots,t^N)$。假定$N$个观测变量符合独立同分布(i.i.d.)，则似然函数如下：&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(\mathbf t|\mathbf X, \theta,\beta)=\prod_{n=1}^N\mathcal
N(t^{(n)}|\hat{t}^{(n)},\beta^{-1})
\label{likelihood}
\end{equation}&lt;/p&gt;
&lt;p&gt;为了计算方便，取对数，得到对数似然函数，&lt;/p&gt;
&lt;p&gt;\begin{eqnarray}
  \ln p(\mathbf t|\mathbf X, \theta,\beta)  &amp;amp;=&amp;amp; \sum_{n=1}^N \ln\mathcal
N(t^{(n)}|\hat{t}^{(n)},\beta^{-1}) \\
   &amp;amp;=&amp;amp; \sum_{n=1}^N \ln\mathcal N(t^{(n)}| \theta^{T}\mathbf x^{(n)},\beta^{-1})
\\
   &amp;amp;=&amp;amp;
\frac{N}{2}\ln\beta-\frac{N}{2}\ln2\pi-\frac{\beta}{2}\sum_{n=1}^{N}\{t^{(n)}-
\theta^{T} \mathbf x^{(n)}\}^2
\end{eqnarray}&lt;/p&gt;
&lt;p&gt;下面要做的就是最优化参数$\beta, \theta$使得似然函数取最大值，这就是最大似然方法的含义。等价的我们也可以最小化似然函数的相反数(negative likelihood function,NLL)，则&lt;/p&gt;
&lt;p&gt;\begin{equation}\label{NLL}
  NLL( \theta) = -  \ln p(\mathbf t|\mathbf X, \theta,\beta)
\end{equation}&lt;/p&gt;
&lt;p&gt;可以看出式子的第三部分与最小平方和误差函数形式是一致的，最小平方和误差函数形式如下：&lt;/p&gt;
&lt;p&gt;\begin{equation}\label{sumsquare}
  \mathbb E=\frac{1}{2}\sum_{n=1}^{N}\{t^{(n)}- \mathbf x^{(n)} \theta\}^2
\end{equation}&lt;/p&gt;
&lt;p&gt;这也从概率的角度解释了最小平方误差函数。下面看一下最优化似然函数的方法：解析法(Normal Equation)和数值计算方法，例如梯度下降(gradient
descent)，随机梯度下降(stochastic gradient descent)。&lt;/p&gt;
&lt;h4&gt;解析法(Normal Equation)}&lt;/h4&gt;
&lt;p&gt;首先来看以下上面式子关于$\beta,\theta$ 的导数。上式关于$\beta$
的一次函数，关于$\theta$的二次函数，可以采用求导的方法来得到一个最大值。分别就似然函数相对于参数$\theta,\beta$的偏导数：&lt;/p&gt;
&lt;p&gt;\begin{eqnarray}
% \nonumber to remove numbering (before each equation)
\frac{\partial}{\partial \theta}\ln(p) &amp;amp;=&amp;amp;
\frac{\beta}{2}\sum_{n=1}^{N}\{(t^{(n)} - \theta^{T} \mathbf x^{(n)})(\mathbf
x^{(n)})\} \
   &amp;amp;=&amp;amp; \frac{\beta}{2} \mathbf X^T( \mathbf t-  \mathbf X  \theta)
\end{eqnarray}&lt;/p&gt;
&lt;p&gt;\begin{equation}
\frac{\partial}{\beta}\ln(p) =
\frac{N}{2\beta}-\frac{1}{2}\sum_{n=1}^{N}\{t^{(n)}- \theta^{T}\mathbf
x^{(n)}\}^2
\end{equation}&lt;/p&gt;
&lt;p&gt;然后让表达式都为0,得到一个二元方程组。解这个二元方程组，得到使最大的参数
$\theta_{ml},\beta_{ml}$。对于$\theta$可以得到等式，&lt;/p&gt;
&lt;p&gt;\begin{equation}
  \mathbf X^T\mathbf X  \theta =\mathbf X^T\mathbf t
\end{equation}&lt;/p&gt;
&lt;p&gt;上面的等式叫做正规方程(norm equation)。通过上面的式子可以看到，可以很容易使用矩阵计算，求出$\theta$的封闭解，结果如下：&lt;/p&gt;
&lt;p&gt;\begin{equation}\label{ana-solution}
   \theta_{ml}=(  \mathbf X^{T}  \mathbf X)^{-1}\mathbf X^{T}  \mathbf t
\end{equation}&lt;/p&gt;
&lt;p&gt;其中：$\mathbf X^{\dag} =(\mathbf X^{T}\mathbf X)^{-1}\mathbf X^{T}$是矩阵$\mathbf
X$的伪逆(Moore-Penrose pesudo-inverse)。这个式子比较重要，这里注意两个问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;解是否存在。通过上面式子可以看到，要求出$(\mathbf X^{T}\mathbf X)^{-1}$，则要求$\mathbf X^{T}\mathbf
X$必须是满秩的。因为$\mathbf X$是一个$D\times m$维的矩阵，所以$\mathbf X^{T}\mathbf X$是$D\times
D$维的矩阵，所以必须要求$\mathbf X^{T}\mathbf X$的秩是$D$，所以$\mathbf X$的秩是$D$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;计算复杂度问题。上面的等式要进行三次矩阵乘法运算，一次矩阵求逆运算，时间复杂度是$O(mD^2+m^2D+D^3)$，。所以当特征向量的维度很大或是数据
集很大时，对矩阵的乘法操作和求逆操作。其中，大部分矩阵乘法操作的时间复杂度是$O(n^{(2+d)})$，求逆运算的时间复杂度是$O(n^{(2+d)})$。&lt;/p&gt;
&lt;p&gt;:::python
var = np.load('reg.npz')
x = var['x']
y = var['y']&lt;/p&gt;
&lt;h1&gt;predict the data&lt;/h1&gt;
&lt;p&gt;X = np.vstack((np.ones(x.shape[0]),x)).T &lt;/p&gt;
&lt;h1&gt;implement least square algorithm&lt;/h1&gt;
&lt;p&gt;theta_best = np.linalg.pinv(X).dot(y)
y_pred = X.dot(theta_best)&lt;/p&gt;
&lt;h1&gt;plot&lt;/h1&gt;
&lt;p&gt;plt.plot(x, y, '.')
plt.plot(x, 3*x, '--',color='r')
plt.plot(x,y_pred,'.', color='k')
plt.plot(x, y_pred, color='g')
plt.show()&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="png" src="/images/c_3_0.png" /&gt;&lt;/p&gt;
&lt;h4&gt;梯度下降(Greadient Descent)&lt;/h4&gt;
&lt;p&gt;在上面的分析中，使用解析的方法直接求解，对于大数据来说是非常耗时的，所以可以采用数值分析的技术来优化目标函数，从而提高程序的可扩展性。本节讨论梯度下降算法。梯度
下降算法基于函数在某点$x$ 处可微，那么函数在$x$
处沿着梯度相反的方向下降最快的原理来寻找最优的参数值。在机器学习算法中，最小化的目标函数通常具有下面的形式：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathbf{J}(\theta)=\sum_{n=1}^{N}\mathbf{J}_i(\theta)
\label{obj_gen}
\end{equation}&lt;/p&gt;
&lt;p&gt;梯度下降算法的过程很简单：首先选取一个初始的参数值$ \theta= \theta_0$，然后使用下面的公式不断迭代，更新参数$ \theta$：&lt;/p&gt;
&lt;p&gt;\begin{equation}
  \theta =  \theta - \eta\bigtriangledown \mathbf{J}( \theta) =
\theta-\eta\sum_{n=1}^{N}\bigtriangledown \mathbf{J}_i( \theta)
\label{gradient}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中，$\eta$称为学习速率，$\eta&amp;gt;0$。在进行梯度下降的时候需要注意的三个地方是:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;归一化：对训练数据进行归一化，可以提高学习的速度，减少迭代次数。&lt;/li&gt;
&lt;li&gt;学习过程的曲线：可以通过观察每次的迭代的目标函数值来观察是否收敛。&lt;/li&gt;
&lt;li&gt;学习速率的选择：可以按照$(...,10^{-5},10^{-4},10^{-3},10^{-2},10^{-1},1,....)$来进行选择。当不收
敛的时候减少$\eta$的值，通常按照$3$的倍数减少。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;使用梯度下降算法来求解$NLL( \theta)$，则迭代公式为：&lt;/p&gt;
&lt;p&gt;\begin{equation}\label{grad-iter}
   \theta = \theta + \eta \sum_{n=1}^{N}\{(t^{(n)} - \theta^{T} \mathbf
x^{(n)})(\mathbf x^{(n)})\}
\end{equation}
下面是实现梯度的代码：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;train_with_gd&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;theta0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;eta&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;maxiter&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;theta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;theta0&lt;/span&gt;
    &lt;span class="nb"&gt;iter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;cost1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;theta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;theta&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;eta&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;cost2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cost1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;cost2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="nb"&gt;iter&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;maxiter&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;break&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="nb"&gt;iter&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="c"&gt;#print &amp;#39;Iteration: %d, Cost: %f&amp;#39; %(iter, cost2)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;theta&lt;/span&gt;


&lt;span class="n"&gt;var&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;reg.npz&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c"&gt;#predict the data&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vstack&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;
&lt;span class="n"&gt;theta0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;eta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.005&lt;/span&gt;
&lt;span class="n"&gt;epsilon&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1e-6&lt;/span&gt; 
&lt;span class="n"&gt;maxiter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;200&lt;/span&gt;
&lt;span class="n"&gt;theta_best&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_with_gd&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;theta0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;eta&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;maxiter&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y_pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;theta_best&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c"&gt;#plot &lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;.&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;--&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;r&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;.&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;k&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;g&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="/images/c_5_0.png" /&gt;&lt;/p&gt;
&lt;h4&gt;随机梯度下降(Stochastic Gradient Descent)&lt;/h4&gt;
&lt;p&gt;上面的梯度下降算法是一种batch方法，一次迭代的时候使用了所有的数据。随机梯度下降的迭代过程近似为下面形式：&lt;/p&gt;
&lt;p&gt;\begin{equation}
 \theta   =  \theta-\eta \bigtriangledown \mathbf{J}_i( \theta)
\label{sto_gradient}
\end{equation}&lt;/p&gt;
&lt;p&gt;这时候一次迭代只需要一个数据即可，所以是一种在线的方法。算法过程如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="err"&gt;初始化参数向量$\&lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="err"&gt;$，学习速率&lt;/span&gt; &lt;span class="err"&gt;$\&lt;/span&gt;&lt;span class="n"&gt;eta&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;

&lt;span class="err"&gt;重复下面操作，直到收敛；&lt;/span&gt;

   &lt;span class="err"&gt;对训练集进行“洗牌”；&lt;/span&gt;

   &lt;span class="n"&gt;For&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="err"&gt;\&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;

       &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt; &lt;span class="err"&gt;\&lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;   &lt;span class="o"&gt;=&lt;/span&gt;  &lt;span class="err"&gt;\&lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="err"&gt;\&lt;/span&gt;&lt;span class="n"&gt;eta&lt;/span&gt; &lt;span class="err"&gt;\&lt;/span&gt;&lt;span class="n"&gt;bigtriangledown&lt;/span&gt; &lt;span class="err"&gt;\&lt;/span&gt;&lt;span class="n"&gt;mathbf&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;J&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="n"&gt;_i&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="err"&gt;\&lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;其中，“洗牌”的意思就是打乱数据集原来数据的顺序。其中比较常用的洗牌算法有Fisher–Yates洗牌等。与梯度下降类似，学习速率的选择对于算法的结果又很重要的
影响。&lt;/p&gt;
&lt;p&gt;使用随机梯度下降算法来求解$NLL( \theta)$，则迭代公式为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
   \theta = \theta + \eta  (t^{(n)} - \theta^ T \mathbf x ^{(n)})(\mathbf
x^{(n)})
\end{equation}&lt;/p&gt;
&lt;p&gt;该算法被称作最小均方(least-mean-squares,LMS)算法。&lt;/p&gt;
&lt;p&gt;与梯度下降算法相比，随机梯度下降不能保证一定会收敛到局部极小值，但是由于随机下降在身的特点，在对于大规模机器学习有很好的性能，例如具有良好的泛化能力，处理大数据
来说计算效率相对梯度下降更好，越来越受到机器学习社区的重视。在神经网络训练中，随机梯度下降是常用方法。&lt;/p&gt;
&lt;h4&gt;最小二乘的几何解释&lt;/h4&gt;
&lt;p&gt;把机器学习模型和空间集合联系起来，在机器学习中是一种常用的思考方式，比如流行学习(manifold
learning)。这里简单看一下最小二乘的集合解释。模型的预测值与输入的关系有如下关系：&lt;/p&gt;
&lt;p&gt;\begin{equation}\label{lr-mat}
  \hat{\mathbf t} = \mathbf X  \theta
\end{equation}&lt;/p&gt;
&lt;p&gt;根据线性代数的知识知道，预测值$\hat{\mathbf t}$可以看作矩阵$\mathbf
X$的$N$维列空间$\mathcal{S}=Span\{\tilde{\mathbf x_1},\tilde{\mathbf
x_2},...,\tilde{\mathbf x_D}\}$中的一个点，是由这矩阵的$D$个列向量线性组合得到的。目标是找一个$\hat{\mathbf
t}$与$ \mathbf t$的距离最小，即：&lt;/p&gt;
&lt;p&gt;\begin{equation}\label{pro}
  arg\min\|\mathbf t-\hat{\mathbf t}\|_2^2
\end{equation}&lt;/p&gt;
&lt;p&gt;使$\mathbf t-\hat{\mathbf t}$与所有的列向量都正交，即使得$\hat{\mathbf t}$是$\mathbf
t$在$\mathcal{S}$的正交投影即可。所以$\mathbf X^T(\mathbf X  \theta-\mathbf
t)=\mathbf{0}$，从而可以得到正规方程，这就从几何上解释了最小二乘方法。&lt;/p&gt;
&lt;h3&gt;正则化最小二乘(Regularized Least Square)&lt;/h3&gt;
&lt;p&gt;根据第二章的知识知道，使用最小二乘(最大似然方法)进行求解，可能出现过拟合的问题，可以使用正则化方法(Regularization)添加一个惩罚项来拓展线性模型
。 数学公式如下：&lt;/p&gt;
&lt;p&gt;\begin{equation}\label{regularized-lms}
  \mathbb E=\frac{1}{2}\sum_{n=1}^{M}\{t^{(n)}- \mathbf x^{(n)} \theta\}^2 +
\frac{1}{2}\lambda \sum_{j=1}^{D}|\theta_j|^q
\end{equation}&lt;/p&gt;
&lt;p&gt;常见的有两种：Ridge 和 Lasso。其中，对于Ridge方法来说，$q=2$；对于Lasso方法来说，$q=1$，$\lambda$是用来平衡这两项的参数
。可以看出两种方法只是惩罚项不同，Ridge方法添加了一个$\ell_2$ 惩罚项，而Lasso方法则添加了一个$\ell_1$
惩罚项。不同$q$对应的正则项示意图，如下：&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/3-4.png", height=200pt, width=800pt&gt;&lt;/p&gt;
&lt;p&gt;Lasso有一个很好的性质就是它可以得出一个稀疏模型（sparse model），参数中很多成分都变成$0$，从而相应的基函数就不再起作用。尤其对高维数据来说，
如果模型是稀疏的，会大大的减少计算量。关于为什么会Lasso容易产生稀疏解，有一个直观的解释。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;cost_fun&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;W&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;W&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
    &lt;span class="n"&gt;E&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;W&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;W&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;

&lt;span class="n"&gt;C&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nb"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;k--&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;l1 norm&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nb"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;k--&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;k&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;l2 norm&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;k&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;mu&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;Sigma&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;1.5&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mf"&gt;1.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="n"&gt;w0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;w1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;w0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;w1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;meshgrid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;w1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;W&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;w1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;W&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;w0&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;w1&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;  
&lt;span class="n"&gt;W&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;W&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cost_fun&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;W&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contour&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;w1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;,[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.4&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="/images/c_7_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;对最小平方和误差函数添加一个正则项，根据拉普拉斯乘数法，就相对的添加了一个约束项，则满足条件的解都应满足约束信息，所以从图中可以看出Lasso更容易产生稀疏解，
因为它与目标函数相交更容易集中在边角上。&lt;/p&gt;
&lt;p&gt;还可以把$\ell_2$ 惩罚项，$\ell_1$ 惩罚项都添加到损失函数上进行惩罚，这种方法称为“Elastic Net”，数学公式如下：&lt;/p&gt;
&lt;p&gt;\begin{equation}\label{elasticnet}
\mathbb E=\frac{1}{2}\sum_{n=1}^{M}\{t^{(n)}- \mathbf x^{(n)} \theta\}^2 +
\frac{1}{2}\lambda_1 \sum_{j=1}^{D}|\theta_j|+\frac{1}{2}\lambda_2
\sum_{j=1}^{D}\theta_j^2
\end{equation}&lt;/p&gt;
&lt;p&gt;前面已经知道最小二乘法等价于最大似然估计，根据在第二章第(8)节的讨论，最大后验估计的目标函数就是在最大似然估计后面添加一个参数的先验，可以看出乘法的最小二乘法
就等价于最大后验估计。Ridge方法添加了$\ell_2$
惩罚项，因为高斯分布是一个指数平方函数，所以可以看出在Ridge方法等价于先验是高斯，条件分布也是高斯下的最大后验估计。下面看一下具体的推导。假设$p(
\theta)=\mathcal{N}( \theta|0,\alpha^{-1}\mathbf{I})$，则&lt;/p&gt;
&lt;p&gt;\begin{eqnarray}
% \nonumber to remove numbering (before each equation)
  \nonumber \mathbf{J}&amp;amp;=&amp;amp; arg\max_\theta \sum_{i=1}^{m}\ln p(\mathbf x_i|
\theta) + \ln p( \theta)\\
  &amp;amp;=&amp;amp; -\frac{\beta}{2}\sum_{n=1}^{N}\{t^{(n)}- \theta^{T} \mathbf x^{(n)}\}^2
-\frac{\alpha}{2} \theta^T \theta + const
\end{eqnarray}
可以看出上面的式子等价于正则化的最小二乘法，对应的惩罚项系数是$\lambda=\frac{\alpha}{\beta}$。同样的道理，可以知道Lasso方法的
先验分布是拉普拉斯分布。&lt;/p&gt;
&lt;h3&gt;偏差-方差平衡(Bias-variance Tradeoff)&lt;/h3&gt;
&lt;p&gt;我们已经知道最大似然估计在数据集小，模型复杂选取复杂时有严重的过拟合现象。通过添加惩罚项，可以在一定程度上防止过拟合，但是随之而来的问题就是如何选取正则化参数$
\lambda$，使得模型能比较好的泛化能力。这其实涉及到了模型复杂度的问题，要具有好的泛化能力，需要模型具有恰当的复杂度。模型复杂度的一种理解方式是&lt;strong&gt;偏差-
方差平衡(Bias-variance Tradeoff)}&lt;/strong&gt;。下面以回归模型讨论偏差-方差平衡，不过其他问题的思想是一致的。期望平均损失可以写成下面的形式：&lt;/p&gt;
&lt;p&gt;\begin{equation}
  \mathbb E[L] = \mathbb E_{p(\mathbf x)}[\{y(\mathbf x)-h(\mathbf
x)\}^2]+\mathbb E_{p(\mathbf x)}[\{h(\mathbf x)-t\}^2]
\end{equation}&lt;/p&gt;
&lt;p&gt;从上面看出，只有第一部分与模型的选取有关，如果用带参数的函数$y(\mathbf x, \theta)$来近似真实的函数$h(\mathbf
x)$，而且在模型训练时知道的是数据集$\mathbf D$，则&lt;/p&gt;
&lt;p&gt;\begin{align}
   \mathbb E_{\mathbf D}[\{y(\mathbf x;\mathbf D)-h(\mathbf x)\}^2] = &amp;amp;
\mathbb E_{\mathbf D}[\{y(\mathbf x;\mathbf D)-\mathbb E_{\mathbf D}[y(\mathbf
x;\mathbf D)]+\mathbb E_{\mathbf D}[y(\mathbf x;\mathbf D)]-h(\mathbf x)\}^2]
\notag \\ = &amp;amp;
   \begin{aligned}[t]
   \mathbb E_{\mathbf D}[\{y(\mathbf x;\mathbf D)-\mathbb E_{\mathbf
D}[y(\mathbf x;\mathbf D)]\}^2]+\mathbb E_{\mathbf D}[\{\mathbb E_{\mathbf
D}[y(\mathbf x;\mathbf D)]-h(\mathbf x)\}^2]\\
   +2\mathbb E_{\mathbf D}[\{y(\mathbf x;\mathbf D)-\mathbb E_{\mathbf
D}[y(\mathbf x;\mathbf D)]\}\{\mathbb E_{\mathbf D}[y(\mathbf x;\mathbf
D)]-h(\mathbf x)\}]\\
     \end{aligned}
    \notag \\
   =&amp;amp;  \mathbb E_{\mathbf D}[\{y(\mathbf x;\mathbf D)-\mathbb E_{\mathbf
D}[y(\mathbf x;\mathbf D)]\}^2]+\{\mathbb E_{\mathbf D}[y(\mathbf x;\mathbf
D)]-h(\mathbf x)\}^2
\end{align}&lt;/p&gt;
&lt;p&gt;从上面的式子看以看出期望平均损失由两项组成：第一项称为方差(variance)是在不同数据集上的结果与在这些数据集上的平均值之间的变化程度，反映了模型对数据集的
敏感程度；第二项是偏差(bias)是在数据集上的平均值与真实值之间的差距。可以写成下面的等式：&lt;/p&gt;
&lt;p&gt;\begin{equation}
  MSE = Variance + Bias^2
\end{equation}&lt;/p&gt;
&lt;p&gt;通常一个灵活或是说是复杂的模型，方差很大，变差很小，一个简单的模型，偏差很大，方差很小，这说明一个好的模型应该是取得偏差和方差的一个平衡，使得它们的和最小
，这叫做偏差-方差平衡。&lt;/p&gt;
&lt;p&gt;下面看一个具体的例子。我们从函数$\cos(2\pi x)$独立的生成100个数据集$\mathbf D_1,\mathbf D_2,...,\mathbf
D_{100}$，每个数据集$\mathbf D_i$包含$25$个点，对所有的这些数据添加高斯噪声，然后使用正则化的基函数线性回归模型进行拟合，使用的基函数是
高斯基函数，为了方便表示，图中的结果使用了$20$个数据集。图中左半部分的三幅图可以反映出方差的情况，右半部分的图可以反映出偏差情况。从图中可以观察到，在最上面
的两幅图中，$\lambda$较大，此时方差较小，但是偏差较大，此时的模型太简单，出现了拟合不足(underfitting)的现象，最下边的两幅图，$\lambda$较大，此时方差较大，但是偏差较小，此时的模型太复杂，出现了过拟合(overfitting)的现象，中间的两幅图反映了方差与偏差取得一个平衡时的情况。最下面的情况中，平均起来的结果与真实情况最接近，但是这样在实际的应用中不是最好的，这是由于在实际的问题中，我们只有1个数据集，而且这个数据集相对于总体来说，还是很小的（可以回忆在第一章第一节中的讨论）。一般来说，高的偏差，代表着模型拟合不足，而高的方差，代表着过拟合。使用不同的正则化系数的正则化线性回归模型结果：从上到下的$
\lambda$逐渐减少，模型变得逐渐复杂。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;linear_model&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;lm&lt;/span&gt;

&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;gendata&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;transdata&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;arg&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;work for only for 1-dims of point in X&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arg&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;
    &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arg&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;arg&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;
    &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arg&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;arg&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;arg&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;  
    &lt;span class="n"&gt;X_t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="c"&gt;#print c.shape&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; &lt;span class="c"&gt;# first is fixed to 1&lt;/span&gt;
        &lt;span class="n"&gt;X_t&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;X_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;

&lt;span class="n"&gt;lambs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;span class="n"&gt;n_dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;
&lt;span class="n"&gt;n_showset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;
&lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;25&lt;/span&gt;
&lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;25&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;nfig&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lamb&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lambs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lambs&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;nfig&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;nfig&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;r&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;avg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros_like&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_dataset&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gendata&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;transdata&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Ridge&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;lamb&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;para_best&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;coef_&lt;/span&gt;
        &lt;span class="n"&gt;y_pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;transdata&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;para_best&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;avg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;avg&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;=&lt;/span&gt;&lt;span class="n"&gt;n_showset&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;g&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lambs&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;nfig&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;nfig&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;r&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;avg&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;n_dataset&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;g&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="/images/c_9_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;下面就看一下怎么来检测到高方差或是偏差，从而来改进模型，提高性能。学习曲线(learning curve)是检测高偏差或是高方差的一种有效方式。在学习曲线中，横
坐标是数据集的大小，纵坐标是目标函数的值。为了检测到模型是否过拟合或是拟合不足，需要分别画出在训练集和验证集上的学习曲线，然后通过观察学习曲线，得到一个比较直观
的结论。下面看一个简单的例子，数据集包含9个点，特征维数是1。使用线性拟合和多项式拟合如图中的数据集中的点，结果如下图。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;13&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;13&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;p1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;poly1d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;polyfit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;p3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;poly1d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;polyfit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;p9&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;poly1d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;polyfit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;xp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ro&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;p1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xp&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;r-&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;degree=1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ro&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;p3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xp&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;b-&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;degree=3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ro&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;degree=9&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;p9&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xp&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;g--&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="/images/c_11_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;多项式拟合结果图，红色圆点代表数据集中的点，从左到右多项式拟合的度数分别为1,3,9。从图中可以看出，度数为1时，出现了拟合不足的现象；而度数为9时则出现了过拟
合的现象。&lt;/p&gt;
&lt;p&gt;但是在实际的应用中，由于问题通常很复杂，特征维度很高，我们几乎不可能画出数据点，然后再画出模型，使用上面的方法观察模型的好坏。不过我们可以使用学习曲线来观察模型
是否具有比较好的泛化能力，即能取得偏差-方差的一个比较好的平衡。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;warnings&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;cost_fun&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;learning_curve&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;degree&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cost_fun&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_val&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_val&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;error&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;error_val&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;x_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;y_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;warnings&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;catch_warnings&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;&lt;span class="c"&gt;#avoid warning output &lt;/span&gt;
            &lt;span class="n"&gt;warnings&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;simplefilter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ignore&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;poly1d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;polyfit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;degree&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; 
        &lt;span class="n"&gt;y_pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;error&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cost_fun&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;y_valpred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_val&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;error_val&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cost_fun&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_val&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_valpred&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;error&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;error_val&lt;/span&gt;

&lt;span class="c"&gt;#np.random.seed(0)&lt;/span&gt;
&lt;span class="n"&gt;degrees&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="c"&gt;#f = lambda x: np.sin(2*np.pi*x)&lt;/span&gt;
&lt;span class="c"&gt;#x = np.random.random(13)&lt;/span&gt;
&lt;span class="c"&gt;#y = f(x) + np.random.normal(size=13,scale=.2)&lt;/span&gt;
&lt;span class="c"&gt;#np.savez(&amp;#39;learn_curve.npz&amp;#39;,x=x,y=y)&lt;/span&gt;
&lt;span class="n"&gt;npz_file&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;learn_curve.npz&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c"&gt;# fix dataset in order the image is consistency.&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;npz_file&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;npz_file&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;x1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;x2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;
&lt;span class="n"&gt;y1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;y2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ro&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;training points&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;bo&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;validate points&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;degree&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;degrees&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;degrees&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;error&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;error_val&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;learning_curve&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;degree&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;cost_fun&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;x1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;x2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;error&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;&lt;span class="n"&gt;error&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;r&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;error_val&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;&lt;span class="n"&gt;error_val&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;degree=&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;degree&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="/images/c_13_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="/images/c_13_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;上面的模型对应的学习曲线图下面的事宜图所示：从图中可以看出，多项式拟合时，在这个例子中，度数为1的时候，在训练集和测试集上的错误较低，而度数为3,10时，训练集
的曲线和测试集的曲线最后差别大，训练集很小，测试集很大，此时的方差大，说明出现了过拟合的现象，借助学习曲线成功的观察到了模型的好坏。这个例子还需要注意两点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;当然，在这个例子中，数据点的数目较小，数据很难反映出真实的情况：我们是从$sin(2\pi x)$进行的采样，但是如果只看蓝色的训练点，则很明显用直线更好。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;从现有的数据中，拿出一部分数据做验证集，可能会使得到的模型很差，尤其是在数据点较少的情况下。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果通过学习曲线，观测到模型出现了过拟合和欠拟合的现象，应该怎么来调整，使得学习算法取得一个好的性能，从而能够胜任它应该完成的任务呢？通常可以有下面的策略来进行
调整：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于过拟合的模型来说（方差很大），可以通过收集更多的数据或是进行特征选择，或是增大正则化系数来进行调整；&lt;/li&gt;
&lt;li&gt;对于欠拟合的模型来说（偏差很大），单纯收集更多的数据并不会使得模型变得更好，通常需要增加更多有意义的特征，或是减小正则化的系数来进行调整。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;贝叶斯线性回归(Bayesian Linear Regression)&lt;/h3&gt;
&lt;p&gt;由前面的知识知道，最大似然估计存在过拟合现象，而最大后验估计(正则化最小二乘)则存在选择模型复杂度困难的问题，本节介绍贝叶斯线性回归，可以自动的进行模型的选择。
由第二章讨论的贝叶斯估计的原理知道，贝叶斯估计首先估计参数的后验分布，然后目标分布对后验分布进行积分（求和），得到预测分布。&lt;/p&gt;
&lt;h4&gt;参数后验分布&lt;/h4&gt;
&lt;p&gt;为了简便，假定参数$\beta$是已知的，先验高斯分布的均值为$0$。使用前面提到的共轭先验分布$p( \theta|\alpha)=\mathcal{N}(
\theta|0,\alpha^{-1}\mathbf{I})$，似然函数(likelihood) 可以写成
$
  p(\mathbf t|\mathbf X, \theta,\beta)= \mathcal N(\mathbf t|\mathbf X
\theta,\beta^{-1}\mathbf{I})
$
，后验分布为，&lt;/p&gt;
&lt;p&gt;\begin{eqnarray}
% \nonumber to remove numbering (before each equation)
  \nonumber  p( \theta|\mathbf X,\mathbf t,\alpha,\beta) &amp;amp;=&amp;amp;  \frac{p(
\theta|\alpha)p(\mathbf t|\mathbf X, \theta,\beta)}{\int p(
\theta|\alpha)p(\mathbf t|\mathbf X, \theta,\beta) d  \theta} \\
    &amp;amp;\propto&amp;amp;  p( \theta|\alpha)p(\mathbf t|\mathbf X, \theta,\beta)
\end{eqnarray}&lt;/p&gt;
&lt;p&gt;则连个高斯分布的乘积依然为高斯分布，下面通过使用高斯分布指数部分的平方项来确定对应的均值和方差：
\begin{eqnarray}
% \nonumber to remove numbering (before each equation)
\nonumber \ln p( \theta|\mathbf X,\mathbf t,\alpha,\beta) &amp;amp;\propto&amp;amp; \ln p(
\theta|\alpha)+ \ln p(\mathbf t|\mathbf X, \theta,\beta)  \\
\nonumber &amp;amp;=&amp;amp; -\frac{1}{2}( \theta)^T\alpha\mathbf I \theta-\frac{1}{2}(\mathbf
t-\mathbf X \theta)^T\beta(\mathbf t-\mathbf X \theta)+const\\
&amp;amp;=&amp;amp; -\frac{1}{2} \theta^T(\alpha\mathbf I+\beta\mathbf X\mathbf X^T) \theta+
\theta^T\mathbf X^T\beta\mathbf I\mathbf t+const
\end{eqnarray}&lt;/p&gt;
&lt;p&gt;对一个标准的高斯分布$\mathcal{N}(\mathbf x| \mu,\Sigma)$，指数部分可以写成
\begin{equation}
  -\frac{1}{2}(\mathbf x- \mu)^T\Sigma^{-1}(\mathbf x- \mu)=-\frac{1}{2}\mathbf
x^T\Sigma^{-1}\mathbf x+\mathbf x^T\Sigma^{-1} \mu + const
\end{equation}&lt;/p&gt;
&lt;p&gt;其中均值与方差为：&lt;/p&gt;
&lt;p&gt;\begin{eqnarray}
% \nonumber to remove numbering (before each equation)
  \mathbf m_{post} &amp;amp;=&amp;amp;  \beta\mathbf S_{post} \mathbf X^T\mathbf t\\
  \mathbf S_{post} &amp;amp;=&amp;amp; (\alpha\mathbf{I}+\beta\mathbf X\mathbf X^T)^{-1}
\end{eqnarray}&lt;/p&gt;
&lt;p&gt;所以参数的后验分布为：
\begin{equation}\label{post}
  \nonumber  p( \theta|\mathbf X ,\mathbf t,\alpha,\beta) = \mathcal{N}(
\theta|\mathbf m_{post},\mathbf S_{post})
\end{equation}&lt;/p&gt;
&lt;p&gt;下面看一个贝叶斯学习的简单例子。使用函数$f(x,\mathbf{a})=a_0+a_1x+\epsilon$生成数据，其中$x\sim
Uniform(-1,1)$，$\epsilon \sim \mathcal{N}(\epsilon|0,0.04)$。在学习过程中假设超参数$\alpha,\
beta$是已知的，分别为$\alpha=2,\beta=20$。中分别左边一列是是似然函数，中间一列是参数分布，最右边一列是数据空间，有观测到的数据和从参数分
布中的采样。在第一行，因为此时没有观察到任何数据，似然函数是空的，由于我们假设的高斯分布的方差是与单位矩阵成正比的(isotropic matrix)，所以从参
数空间中采样得到的参数在空间中每个方向都有。在第二行中，当观测到第一数据点时(在最右边用蓝色的圆圈表示)，左边似然函数就穿过了该数据点，参数的后验为先验乘以似然
，即第一行中的参数分布乘以第二行中的似然函数，可以看到此时参数的后验分布变成了“山脊”的形状，与先验有了明显的不同，此时再从参数分布中采样，从第二行的右图中可以
看到参数在向观测的数据点靠拢。在第三行中，当观测到第二个数据点时，左边表示该点的似然函数穿过了该数据点，参数的后验为先验乘以似然，即第二行中的参数分布乘以第三行
中的似然函数，可以看到此时参数的后验分布已经变得的相对集中，靠近真实参数（用白色十字号表示），第四行表示观测了20个点后的情况，看以看到此时的后验分布已经变得非
常集中，而且非常靠近真实的参数。当观测到无穷多个点时，后验分布就会变成以真实参数为中心的一个$\delta$函数。从这个过程可以看出贝叶斯学习的序列化学习特性，
不断增加的数据，可以改变参数的分布形式，增加对参数的认识。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt; 
&lt;span class="kn"&gt;import&lt;/span&gt;  &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt; 
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;numpy.linalg&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;lstsq&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;slogdet&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;eig&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;scipy.stats&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;norm&lt;/span&gt;

&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="n"&gt;w0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.3&lt;/span&gt;
&lt;span class="n"&gt;w1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.3&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;true_w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;w0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;w1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="n"&gt;beta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;25&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;mvn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Sigma&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;implement PDF for multivariate normal distribution.&lt;/span&gt;
&lt;span class="sd"&gt;       Parameters:&lt;/span&gt;
&lt;span class="sd"&gt;         - X: n * D matrix&lt;/span&gt;
&lt;span class="sd"&gt;         - mu: mean vector&lt;/span&gt;
&lt;span class="sd"&gt;         - Sigma: covariance matrix&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;Xm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;
    &lt;span class="n"&gt;logv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Xm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;lstsq&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Sigma&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Xm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;slogdet&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Sigma&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;logv&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;plot_contour&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;x1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;meshgrid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;x1&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;y1&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;  
    &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contourf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;256&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;true_w&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;true_w&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;w+&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;ms&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;plot_sample_lines&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;w1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;example&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;w0&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;w0&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;w1&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;example&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;example&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;example&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ro&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;bayes_update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sigma_prior&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mu_prior&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;beta&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;sigma&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sigma_prior&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;beta&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;mu&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sigma_prior&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_prior&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;beta&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;

&lt;span class="n"&gt;mu_prior&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;sigma_prior&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eye&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;
&lt;span class="c"&gt;# plot first row&lt;/span&gt;
&lt;span class="n"&gt;n_row&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="n"&gt;n_fig&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_fig&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;n_fig&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="n"&gt;post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;mvn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mu_prior&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;sigma_prior&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plot_contour&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c"&gt;#show sample &lt;/span&gt;
&lt;span class="n"&gt;w0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;w1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;multivariate_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_prior&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;sigma_prior&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_fig&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;n_fig&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="n"&gt;plot_sample_lines&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;w1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;n_row&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c"&gt;#show likelihood&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_fig&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;n_fig&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pdf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]])),&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;plot_contour&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;likelihood&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="c"&gt;#show posterior&lt;/span&gt;
        &lt;span class="c"&gt;#print mu_prior&lt;/span&gt;
        &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;bayes_update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sigma_prior&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mu_prior&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;beta&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]]]),&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
        &lt;span class="n"&gt;sigma_prior&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;
        &lt;span class="n"&gt;mu_prior&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;
        &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_fig&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;n_fig&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="n"&gt;post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;mvn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;plot_contour&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="c"&gt;#show sample &lt;/span&gt;
        &lt;span class="n"&gt;w0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;w1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;multivariate_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;
        &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_fig&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;n_fig&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="n"&gt;plot_sample_lines&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;w1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]))&lt;/span&gt;
        &lt;span class="n"&gt;n_row&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c"&gt;#print mu_prior&lt;/span&gt;
        &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;bayes_update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sigma_prior&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mu_prior&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;beta&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]]]),&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
        &lt;span class="n"&gt;sigma_prior&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;
        &lt;span class="n"&gt;mu_prior&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="/images/c_15_0.png" /&gt;&lt;/p&gt;
&lt;h4&gt;预测分布&lt;/h4&gt;
&lt;p&gt;在第二章中的贝叶斯估计中已经讨论过，如何求预测分布：通过对参数后验分布求积分，得到在给定数据情况下，对新数据的条件分布。公式如下：
\begin{equation}\label{bayesian}
  p(t|\mathbf x,\mathbf X,\mathbf t) = \int_\theta p(t|\mathbf x,
\theta,\beta)p( \theta|\mathbf X,\mathbf t,\alpha,\beta)d \theta
\end{equation}
可以得到，$ p(t|\mathbf x, \theta,\beta)$与$p( \theta|\mathbf X,\mathbf
t,\alpha,\beta)$服从高斯分布，参照求解后验分布，两个高斯分布的乘积的依然是一个高斯分布，然后对$
\theta$求积分，依然为高斯分布，均值与方差如下：&lt;/p&gt;
&lt;p&gt;\begin{eqnarray}
% \nonumber to remove numbering (before each equation)
 \mathbf m_{pred} &amp;amp;=&amp;amp;  \mathbf x^T \mathbf m_{post} \\
  \mathbf S_{pred} &amp;amp;=&amp;amp;  \frac{1}{\beta}+ \mathbf x^T \mathbf S_{post}\mathbf x
\end{eqnarray}
所以预测分布为：
\begin{equation}
  \nonumber    p(t|\mathbf x,\mathbf x,\mathbf t) = \mathcal N( \theta|\mathbf
m_{pred},\mathbf S_{pred})
\end{equation}&lt;/p&gt;
&lt;p&gt;可以看出预测分布的方差包含两部分：（1）数据的噪声$\beta$；（2）参数的方差$\mathbf
S_{post}$，反映了参数的不确定。在上面小结已经讨论过了，当有新的观测数据的话，后验分布就会变的更加集中，所以一般来说，$\mathbf
S_{pred}^{N+1} &amp;lt; \mathbf S_{pred}^N$。在$N \rightarrow \infty
$的极限情况下，方差第二部分就会消失，此时预测分布的方差只是由数据的噪声$\beta$来决定。&lt;/p&gt;
&lt;h3&gt;引用&lt;/h3&gt;
&lt;p&gt;[1]. &lt;a href="https://class.coursera.org/ml-004}"&gt;machine learning&lt;/a&gt;, Adrew Ng, 2013.&lt;/p&gt;
&lt;p&gt;[2]. &lt;a href="http://en.wikipedia.org/wiki/Rank_(linear_algebra)"&gt;linear algebra&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[3]. The tradeoffs of large scale learning,  L Bottou, 2007.&lt;/p&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</summary><category term="Machine Learning"></category></entry><entry><title>概率知识简介</title><link href="/posts/2014/05/pr_introduce/" rel="alternate"></link><updated>2014-05-02T01:00:00+08:00</updated><author><name>Webdancer</name></author><id>tag:,2014-05-02:posts/2014/05/pr_introduce/</id><summary type="html">&lt;p&gt;机器学习中，遇到的一个很关键的问题就是不确定性，可能不同的人对事物不确性的理解存在不同。对于不确性有两类认识：1)事物本身就是不确定性的，所以其背后的规律也就是不确定性的；2)事物本身是确定的，但由于人类认识的限制，所以需要用不确定的规律。典型的例子就是“量子”，波尔认为量子规律本身就是不确定的，而爱因斯坦则认为量子是确定的，量子的不确定性是人类认识的限制。概率论为解决不确定性问题提供了一个系统的框架，因此概率是机器学习问题中需要掌握的基础知识。机器学习可以分为了两个阶段，第一个阶段是推理(inference),得到相关的概率；第二阶段根据推理阶段得到的概率，使用决策论知识做出最优的决策。本章论述概率论的基本知识。&lt;/p&gt;
&lt;h3&gt;概率&lt;/h3&gt;
&lt;p&gt;概率论就是研究不确定现象的数学。举一个例子，做投掷一个色子的随机试验，每次试验点数可能为$1,2,3,4,5,6$，随机试验的结果，称为随机变量(random variable)，记作$X$。简单的说，随机变量就是可能样本输出空间的一个函数。随机变量的取值范围称为样本空间(sample space)，记作$\Omega$，在这个例子中，$\Omega=\{1,2,3,4,5,6\}$，样本空间的子集$A$称为事件(event)，比如出现点数为偶数。概率是对随机事件发生可能性的度量，满足一定的条件。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;概率的定义如下：如果一个函数$p:S\to\mathbb{R}, A\to
p(A)$指定给每一个事件空间$\Omega$中的事件$A$一个实数$P(A)$,满足以下三条公理:
 \begin{eqnarray}
  \nonumber 0 &amp;lt;= p(A) &amp;lt;= 1; \\
  \nonumber  P(S) = 1 ; \\
   P(A \cup B) = P(A) + P(B),if P(A\cap B)=0.
   \label{pro}
\end{eqnarray}
那么函数$P$叫做概率函数，相应的$P(A)$就是事件$A$的概率。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;随机变量与其他的数学变量不同，它的取值不是确定的，有多种可能，比如普通的变量，在下一个取值只能是确定的。注意随机变量和随机变量取值的区别，在上面的投掷色子的例子
中，$X$是随机变量，包含可能所有的试验结果，随机变量的某个取值可能为$1,2,3,4,5,6$中的任何一个，记作$x$，比如随机变量取$x$的概率可以记作$p
(X=x)$。在下面的例子中，为了表示的简单，$p(X)$表示随机变量的分布，$p(x)$表示随机变量分布在某个特定点的值。
根据样本空间的类型，随机变量有离散的和连续的两种基本类型。样本空间如果是有限的或是无限可数的，则称该随机变量为离散型随机变量，否则称为连续型随机变量。&lt;/p&gt;
&lt;p&gt;下面这三个定理是进行概率运算的基石，对后面的概率分析有非常重要的作用。概率推断就是根据下面定律进行算术运算。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;加法定理：
  \begin{equation}\label{sum_rule}
    p(X)=\sum_Yp(X,Y);
  \end{equation}&lt;/li&gt;
&lt;li&gt;乘法定律：
   \begin{equation}\label{product_rule}
     p(X,Y)=p(X)p(Y|X);
   \end{equation}
在上面的两个定理中，$p(X,Y)$是$X,Y$同时发生的联合概率，$p(Y|X)$是给出$X$条件下，$Y$的条件概率，$p(X)$是$X$的边缘概率。两个定
理比较好理解，加法定理告诉我们求一个随机变量的边缘概率，只要对其他的所有随机变量的可能取值求和（或是积分）便可；乘法是一个链式法则，两个变量同时发生的概率，可以
等于一个变量的概率与在该变量给出条件下另一个变量条件概率的乘机。两个定理都可以拓展到三个以及三个变量以上的情况：
\begin{equation}\label{ext_sum_rule}
  p(X_1)=\sum_{X_2,..,X_n}p(X_1,X_2,..,X_n);
\end{equation}
\begin{equation}\label{ext_prod_rule}
  p(X_1,X_2,..,X_n)=p(X_1)p(X_2|X_1)p(X_3|X_1,X_2)...p(X_n|X_1,X_2,...,X_{n-1});
\end{equation}&lt;/li&gt;
&lt;li&gt;贝叶斯定理:
  \begin{equation}\label{bayes}
     p(H|E)=\frac{p(H)p(E|H)}{p(E)}
  \end{equation}
其中$p(H|E)$表示在$E$发生情况下，$H$发生的概率，是一个条件概率。对概率的解释，有两种观点：频率主义和贝叶斯主义。频率主义认为：概率就是频率的极限，而贝叶斯主义则对概率的理解不同：在贝叶斯主义者看来，概率代表的是信任度，贝叶斯定理解释了在一个命题中，在考虑了证据后对信任度的影响；而频率主义者看来，概率代表了事件发生的个数与事件空间总的数目的比值，贝叶斯定理描述了特定事件概率值之间的关系。在贝叶斯解释中，$p(H)$表示的是先验概率(prior)，$H$初始的信任度;$p(E|H)$表示似然函数，$p(H|E)$ 表示的是后验概率(poster)，考虑了$E$后的信任度；$p(E)$ 表示边缘似然，或是称为模型置信度；这个因子对于所有假设都是一样的，可以不用考虑。$p(H)$表示的是先验概率(prior)，$p(E|H)$ 表示似然(likelihood)，$p(H|E)$ 表示的是后验概率(poster)。根据上面的贝叶斯定理，在贝叶斯推断中，可以根据先验概率和似然函数，求出后验概率；得出后验概率可以作为下面继续推断的先验概率。由于在实际的使用中，$E$的概率对于我们的模型没有影响，我们可以省略掉，所以贝叶斯定理也可以表示为：
$$posterior \propto likelihood \times prior$$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于连续类型的随机变量，上面式子中的加号变为积分符号即可，不影响式子的意义。在三个定理中，涉及到了三种不同类型的概率：联合概率$p(X,Y)$，边缘概率$p(X
)$和条件概率$p(X|Y)$。这里假设只有两个随机变量，多个随机变量的情况类似的表示。可以看出这三种分布讨论的是随机变量之间的关系，是机器学习建模最常用的工具
。一般按照变量之间以来的关系，变量之间的关系可以分为：独立(independent)和条件独立(conditional independent)。 随机变量独立
，指的是变量之间没有任何的关系，一个变量的概率大小对另一个变量没有任何影响。随机变量条件独立，关于条件独立的内容在后面的概率图模型中会详细论述，概率图模型是描述
随机变量之间的条件独立关系最常用到工具。指的是给定一个随机变量的情况下，两个变量之间没有任何影响。形式化如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;独立(independent )：
\begin{equation}\label{independent}
  p(X,Y)=p(X)p(Y)
\end{equation}&lt;/li&gt;
&lt;li&gt;条件独立(conditional independent)：
  \begin{equation}\label{cond_independent}
    p(X,Y|Z)=p(X|Z)p(Y|Z)
  \end{equation}&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;概率分布&lt;/h3&gt;
&lt;p&gt;了解概率的基本知识以后，下面看一下概率分布的知识。随机变量有两种：离散型和连续型，所以概率分布也有两种基本类型，离散概率分布和连续概率分布。概率质量函数(pro
bability mass function, PMF)用来描述离散分布；而概率密度函数(probability density function,
PDF)用来描述连续分布；两者非常的不同，在$PMF$中，每个变量$X$的$PMF(X)$都对应一个概率值，即$X$取某个值时的概率；在$PDF$
中，每个变量的对应取值不是概率，只有通过积分，才能得到概率。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;如果随机变量是离散的，即样本空间$\Omega$是有限的或是无限可数的，$X$的概率质量函数(probability mass function,
PMF)$p$ 满足下面的条件：
  $$p(x) \geq 0;$$
  $$\sum_{x\in \Omega} p(x)=1.$$&lt;/p&gt;
&lt;p&gt;如果随机变量$X \in \mathbb{R}$，对于任意的$a,b \in \mathbb{R}$，$X$的概率密度函数(probability
density function, PDF)$f$满足下面的条件：
  $$f(x) \geq 0;$$
  $$\int_{x\in \Omega} f(x)=1.$$&lt;/p&gt;
&lt;p&gt;如果随机变量$X \in \mathbb{R}$，$X$的累计分布函数(cumulative distribution function,
CDF)$F$满足下面的条件：
$$F(x) = \int_{-\infty}^x f(x)dx$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;对于离散分布，可以通过枚举的方式列出概率分布。比如投掷一个硬币，正面朝上，反面朝上的概率相等，都为$\frac{1}{2}$，即$p(X=0)=p(X=1)=\frac{1}{2}$。&lt;/p&gt;
&lt;h3&gt;期望与方差&lt;/h3&gt;
&lt;p&gt;期望是分布函数的一个重要内容，也是涉及到概率时的一个重要操作：对函数求一个加权的均值。对于离散随机变量与连续的随机变量来说，期望求法不同，离散分布如下：
\begin{equation}\label{discrete_expect}
  \mathbb{E}(f)=\sum_{x\in var(X)}p(x)f(x)
\end{equation}
上面的式子的意义就是在函数每个取值求一个加权的均值，而权值是该点的概率。
对于连续分布，期望如下：
\begin{equation}\label{continous_expect}
  \mathbb{E}(f)=\int_{x\in var(X)}p(x)f(x)
\end{equation}
对于上面两种情况，如果我们从分布函数或是密度函数进行采样，得到$N$个样本${x_1,x_2,...,x_N}$，则期望的计算可以近似为：
\begin{equation}\label{approx_expect}
  \mathbb{E}(f)=\frac{1}{N}\sum_{i=1}^{N}f(x_i)
\end{equation}&lt;/p&gt;
&lt;p&gt;方差反映了函数在其均值附近的差异性，定义如下：&lt;/p&gt;
&lt;p&gt;\begin{equation}\label{var}
  Var(f)=\mathbb E ((f(x)-\mathbb E(f))^2)
\end{equation}
方差也可以表示如下：
\begin{equation}\label{var1}
  Var(f)=\mathbb E({f}^2(x))- \mathbb E(f)^2
\end{equation}&lt;/p&gt;
&lt;h3&gt;常见离散分布&lt;/h3&gt;
&lt;p&gt;本节给出一些常见的连续分布。&lt;/p&gt;
&lt;h4&gt;伯努利分布(Bernoulli distribution)}&lt;/h4&gt;
&lt;p&gt;在投1次硬币的随机试验中，定义随机变量$X$为正面朝上的次数，则样本空间为${0,1}$，设正面朝上的概率$p(X=1|\mu)=\mu$，则$
P(X=0)=1-P(X=1)$，则出现$X=m$次正面朝上的分布可以写成：&lt;/p&gt;
&lt;p&gt;\begin{equation}\label{bern}
 p(x|\mu)=\mu^x(1-\mu)^{(1-x)}
\end{equation}&lt;/p&gt;
&lt;p&gt;称$X$服从参数为$\mu$的伯努利分布(Bernoulli distribution)，记作$X\sim
Bern(\mu)$，其中$\mu$为一次试验中正面朝上的概率。其均值和方差的公式：&lt;/p&gt;
&lt;p&gt;\begin{equation}
  \mathbb{E}(x) = \mu
\end{equation}&lt;/p&gt;
&lt;p&gt;\begin{equation}
  var(x) = \mu(1-\mu)
\end{equation}&lt;/p&gt;
&lt;h4&gt;二项分布(binomial distribution)&lt;/h4&gt;
&lt;p&gt;在一个投$N$次硬币的随机试验中，定义随机变量$X$为正面朝上的次数，则样本空间为${0,1,2,3,...,N}$，则出现$X=m$次正面朝上的分布可以写
成：&lt;/p&gt;
&lt;p&gt;\begin{equation}\label{bin}
  p(m|N,\mu)=\left(
               \begin{array}{c}
                 N \\
                 m \\
               \end{array}
             \right)\mu^m(1-\mu)^{N-m}
\end{equation}&lt;/p&gt;
&lt;p&gt;称$X$服从参数为$N,\mu$的二项分布(binomial distribution)，记作$X\sim
Bin(N,\mu)$，其中$\mu$为一次试验中正面朝上的概率，&lt;/p&gt;
&lt;p&gt;\begin{equation}\label{combinations}
  \left(
               \begin{array}{c}
                 N \\
                 m \\
               \end{array}
             \right)=\frac{N!}{(N-m)!m!}
\end{equation}
是从$N$个硬币中选择$m$个正面朝上的组合方式。其均值和方差的公式：&lt;/p&gt;
&lt;p&gt;\begin{equation}
  \mathbb{E}(m) = N\mu
\end{equation}&lt;/p&gt;
&lt;p&gt;\begin{equation}\label
  var(m) = N\mu(1-\mu)
\end{equation}&lt;/p&gt;
&lt;p&gt;可以看出伯努利分布是二项分布$N=1$时的一个特例。下面看一下当$N=10$，而$\mu$分别取0.25,0.5,0.75的例子。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;scipy.stats&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;binom&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;IPython.core.pylabtools&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;figsize&lt;/span&gt;


&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;
&lt;span class="n"&gt;p_s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.25&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.75&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p_s&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p_s&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;m&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;$\mu= &lt;/span&gt;&lt;span class="si"&gt;%0.2f&lt;/span&gt;&lt;span class="s"&gt;$&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;rvx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;binom&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rvs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rvx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bins&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;normed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;blue&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="/images/b_1_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;从上图显示的情况，可以很清楚的看出：在参数$\mu$较小的时候，硬币朝上出现的次数较少，在$\mu=0.25$时时左偏的，但是虽然$\mu$增大，则硬币朝上出现
的次数则在增加。&lt;/p&gt;
&lt;h4&gt;类别分布(Category distribution)&lt;/h4&gt;
&lt;p&gt;伯努利分布可以很好的刻画像投掷一次硬币这样有两个结果的随机试验，但是对于投掷色子这样的试验，结果不是二值的，所以不能用伯努利分布来刻画。可以将有两种互斥的状态的伯努利分布扩展成有$K$种互斥的状态的类别分布。例如，在投掷色子的试验中有$6$
种互斥的状态${1,2,...,6}$。为了表示时方便，可以对随机变量如下编码：用$K$维的向量表示随机变量的取值，当第$k$
个时间发生时，向量的第$k$位为$1$，其他位为$0$。这种编码方式称为1-of-K(也称1-of-shot)编码。例如，在$K=6$时，样本空间为$\Omega={1,2,...,6}$，现在表示为$\Omega={(1,0,0,0,0,0),(0,1,0,0,0,0),...,(0,0,0,0,0,1)}
$。概率表示相应的可以表示为：$p(X=1)=p(X=\mathbf{x})=p(X=(1,0,0,0,0,0))$。在进行$1-of-K$编码后，则类别概率分
布可以写成如下形式：&lt;/p&gt;
&lt;p&gt;\begin{equation}\label{cat_distribution}
  p(\mathbf{x}|\mathrm{\mu})=\prod_{k=1}^K \mu_k^{x_k}
\end{equation}&lt;/p&gt;
&lt;p&gt;称$X$服从参数为$\mu$的类别分布(category distribution)，记作$X\sim
Cat(\mu)$，其中，参数$\mathrm{\mu}=(\mu_1,\mu_2,...,\mu_K)$ 为$K$
种变量取值的概率，满足$\sum_k\mu_k=1$。&lt;/p&gt;
&lt;h4&gt;多项分布(Multinomial distribution)&lt;/h4&gt;
&lt;p&gt;可以像伯努利分布扩展到二项分布一样，可以将类别分布扩展到多项分布。在一个投$N$次色子的随机试验中，随机变量$X=(x_1,x_2,...,x_K)$，其中$x_i$为$i$面朝出现的次数，满足约束:$\sum_kx_k=N$，则出现$X$
的分布可以写成:&lt;/p&gt;
&lt;p&gt;\begin{equation}\label{multi_distribution}
  p((x_1,x_2,...,x_K)|\mathrm{\mu},N)=\left(\begin{array}{c}
                                            N \\
                                            x_1,x_2,...,x_K
                                          \end{array}
  \right)\prod_{k=1}^K \mu_k^{x_k}
\end{equation}
称$X=(x_1,x_2,...,x_K)$服从参数为$\mu$的多项分布(Multinomial distribution)，记作$X\sim
Mult(\mu,N)$，其中，参数$\mathrm{\mu}=(\mu_1,\mu_2,...,\mu_K)$ 为$K$
种变量取值的概率,且满足$\sum_k\mu_k=1$，
\begin{equation}\label{multinomial coefficient}
  \left(\begin{array}{c}
                                            N \\
                                            x_1,x_2,...,x_K
                                          \end{array}
  \right)=\frac{N!}{x_1!x_2!...x_K!}
\end{equation}&lt;/p&gt;
&lt;h4&gt;泊松分布(Poisson distribution)&lt;/h4&gt;
&lt;p&gt;随机变量$X \in {0,1,2,3,...,}$，则$X$ 的泊松分布分布可以写成:&lt;/p&gt;
&lt;p&gt;\begin{equation}
  p(x|\lambda)= e^{-\lambda}\frac{\lambda^x}{x!}
\end{equation}&lt;/p&gt;
&lt;p&gt;记作$X\sim Poi(\mu,N)$。下面看一下$\lambda = 1,5,10$时，不同的泊松分布。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;scipy.stats&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;poisson&lt;/span&gt;
&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;lams&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lam&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lams&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lams&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;vrx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;poisson&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rvs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lam&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10000000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;$\lambda= &lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s"&gt;$&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;lam&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;hist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vrx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;normed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;blue&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="/images/b_4_0.png" /&gt;&lt;/p&gt;
&lt;h3&gt;常见连续分布&lt;/h3&gt;
&lt;p&gt;本节给出一些常见的连续分布。&lt;/p&gt;
&lt;h4&gt;均匀分布(Uniform distribution)&lt;/h4&gt;
&lt;p&gt;连续型随机变量$X$的均匀分布的密度函数如下：
\begin{equation}
  p(x|a,b)=\left\{
             \begin{array}{ll}
               \frac{1}{b-a}, &amp;amp;  a \leq x \leq b ;  \\
               0, &amp;amp; \hbox{otherwise.}
             \end{array}
           \right.
\end{equation}
记作$x \sim U(a,b)$。&lt;/p&gt;
&lt;h4&gt;高斯分布(guassian distribution)&lt;/h4&gt;
&lt;p&gt;高斯分布(guassian distribution)是也许是我们常见的一种分布形式，也是在机器学习中用的最多的一类分布。&lt;/p&gt;
&lt;p&gt;\begin{equation}\label{guassian}
  \mathcal
N(x|\mu,\sigma^2)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中，$\mu$是均值，$\sigma$是标准差(其中$\sigma^2$是方差)；$p(X=x)=\mathcal
N(x|\mu,\sigma)$可以写成$x \sim \mathcal N(x|\mu,\sigma)$；其均值与方差为：&lt;/p&gt;
&lt;p&gt;\begin{eqnarray}
% \nonumber to remove numbering (before each equation)
  \mathbb{E}(x) &amp;amp;=&amp;amp; \mu \\
  var(x) &amp;amp;=&amp;amp; \sigma^2
\end{eqnarray}&lt;/p&gt;
&lt;p&gt;当$\mu=0,\sigma=1$ 时，称为标准正态分布，也成为贝尔曲线(bell curve)。下面看一下$\mu,\sigma$分别取不同值时，高斯分布的情
况。从图中可以看到$\mu$决定了高斯曲线的位置，而$\sigma$则对应了曲线的形状，$\sigma$越小，则曲线越“高瘦”，反之，则“低矮”。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;scipy.stats&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;norm&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;mu_s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;sigma_s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_s&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sigma_s&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
        &lt;span class="n"&gt;mu&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mu_s&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;sigma&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sigma_s&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_s&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sigma_s&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;$\sigma$= &lt;/span&gt;&lt;span class="si"&gt;%0.2f&lt;/span&gt;&lt;span class="s"&gt;, $\mu$= &lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pdf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="/images/b_6_0.png" /&gt;&lt;/p&gt;
&lt;h4&gt;伽马分布(Gamma distribution)&lt;/h4&gt;
&lt;p&gt;对于随机变量$X&amp;gt;0$，伽马分布如下：&lt;/p&gt;
&lt;p&gt;\begin{equation}
    p(\tau|a,b)=\frac{1}{\Gamma(a)} b^a\tau^{a-1}e^{-b\tau}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中，参数$a,b$满足$a&amp;gt;0,b&amp;gt;0$，$\Gamma(x)$是伽马函数，定义如下：&lt;/p&gt;
&lt;p&gt;\begin{equation}\label{gamma}
   \Gamma(x) = \int_0^\infty u^{x-1}e^{-\mu}d\mu
\end{equation}&lt;/p&gt;
&lt;p&gt;其有一个很好的性质，$\Gamma(x+1)=x\Gamma(x)$。其均值与方差为：&lt;/p&gt;
&lt;p&gt;\begin{eqnarray}
% \nonumber to remove numbering (before each equation)
  \mathbb{E}(\tau) &amp;amp;=&amp;amp; \frac{a}{b} \\
  var(\tau) &amp;amp;=&amp;amp;  \frac{a}{b^2}
\end{eqnarray}&lt;/p&gt;
&lt;p&gt;伽马分布是单变量高斯分布精度（方差$\sigma$的倒数）参数的共轭先验。共轭先验具有一个良好的性质：一个分布乘以该分布的共轭先验得到的后验分布仍然满足该分布。
当$a=1$ 时，伽马分布就变为了指数分布(exponential distribution)，当$b=\frac{1}{2}$时，
可以变成卡方分布(Chi-squared distribution)：$\chi^2(x|\nu)=Gam(x|\frac{1}{\nu},\frac{1}{2})$。下面看一下$b=1$时的伽马分布，红，蓝，绿分别代表$a=1,1.5,2$的情况。从图中可以看出，伽马分布可以在有一个很快的衰减过程，可以用来模拟“长尾”（long tail）现象。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;scipy.stats&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;gamma&lt;/span&gt;

&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;a_s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;1.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a_s&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a_s&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;a=&lt;/span&gt;&lt;span class="si"&gt;%.2f&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;gamma&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pdf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="/images/b_8_0.png" /&gt;&lt;/p&gt;
&lt;h4&gt;学生分布(student's-t distribution)&lt;/h4&gt;
&lt;p&gt;假设有一个单变量的高斯分布$\mathcal{N}(x|\mu,\tau^{-1})$和精度共轭伽马先验分布$Gam(\tau|a,b)$，使用换元积分法，令$
\lambda=\left[b+\frac{(x-\mu)^2}{2}\right]\tau$}，得到学生分布:&lt;/p&gt;
&lt;p&gt;\begin{eqnarray}
% \nonumber to remove numbering (before each equation)
  \nonumber p(x|\mu,a,b) &amp;amp;=&amp;amp; \int_{0}^{\infty}
\mathcal{N}(x|\mu,\tau^{-1})Gam(\tau|a,b)d\tau \\
   \nonumber  &amp;amp;=&amp;amp; \int_{0}^{\infty} \left(\frac{\tau}{2\pi}\right)^{\frac{1}{2}}
\exp{-\frac{\tau}{2}(x-\mu)^2}\frac{1}{\Gamma(a)}b^a\tau^{a-1}e^{-b\tau}d\tau
\\
   &amp;amp;=&amp;amp; \frac{\Gamma(a+\frac{1}{2})}{\Gamma(a)}b^a\left(\frac{1}{2\pi}\right)^{\frac{1}{2}}
   \left[b+\frac{(x-\mu)^2}{2}\right]^{-a-\frac{1}{2}}
\end{eqnarray}&lt;/p&gt;
&lt;p&gt;令$\nu=2a$，$\lambda=\frac{a}{b}$，可以得到下面形式：&lt;/p&gt;
&lt;p&gt;\begin{equation}\label{stu-dist}
  p(x|\mu,\lambda,\nu)=\frac{\Gamma(\nu/2+\frac{1}{2})}{\Gamma(\nu/2)}\left(
  \frac{\lambda}{\pi\nu}\right)^{\frac{1}{2}}\left[1+\frac{\lambda(x-\mu)^2}{\nu}\right]
^{-\nu/2-\frac{1}{2}}
\end{equation}&lt;/p&gt;
&lt;p&gt;称$X$服从参数为$\mu,\lambda,\nu$的学生分布(student's-t distribution)，记作
$X \sim St(\mu,\lambda,\nu)$，其中，$\lambda$是分布的精度，$\nu$是分布的自由度(degrees of freedom)。
当$\nu=1$时，学生分布变成柯西分布(Cauchy distribution)；当$\nu\rightarrow\infty$时，学生分布
变成高斯分布。下面的例子：红，绿，蓝对应的自由度$\nu=0.1,1,\infty$，看以看出相对于高斯分布，学生分布可以的“尾巴”比较大，这样产生的一个好处就是对离群值(outlines) 更好的鲁棒性。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;scipy.stats&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;norm&lt;/span&gt;
&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;nu_s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;red&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;green&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;blue&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nu&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nu_s&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pdf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nu&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;$&lt;/span&gt;&lt;span class="se"&gt;\\&lt;/span&gt;&lt;span class="s"&gt;nu$ =&lt;/span&gt;&lt;span class="si"&gt;%0.2f&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;nu&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pdf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;$&lt;/span&gt;&lt;span class="se"&gt;\\&lt;/span&gt;&lt;span class="s"&gt;nu$ =+$\infty$&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="/images/b_10_0.png" /&gt;&lt;/p&gt;
&lt;h4&gt;拉普拉斯分布(Laplace distribution)&lt;/h4&gt;
&lt;p&gt;对一个随机变量$X$，拉普拉斯分布的密度函数如下:&lt;/p&gt;
&lt;p&gt;\begin{equation}\label{lap-dist}
  p(x|\mu,b)= \frac{1}{2b} \exp{-\frac{\mid x-\mu\mid}{b}}
\end{equation}
 记作$X\sim Lap(\mu,b)$，其中，$\mu$是分布的均值，表示分布的位置，$b$用来做正规化。其均值与方差为：
 \begin{eqnarray}
 % \nonumber to remove numbering (before each equation)
   \mathbb{E}(x) &amp;amp;=&amp;amp; \mu \\
   var(x) &amp;amp;=&amp;amp; \frac{1}{2b^2}
 \end{eqnarray}&lt;/p&gt;
&lt;h4&gt;贝塔分布(Beta distribution)&lt;/h4&gt;
&lt;p&gt;对一个随机变量$X \in [0,1]$，贝塔分布的密度函数如下:&lt;/p&gt;
&lt;p&gt;\begin{equation}
  p(x|a,b)= \frac{\Gamma(a)}{\Gamma(a)\Gamma(b)}x^{a-1}(1-x)^{(b-1)}
\end{equation}
 记作$X\sim Beta(\mu,b)$，其中，$a,b&amp;gt;0$用来做正规化。其均值与方差为：
 \begin{eqnarray}
 % \nonumber to remove numbering (before each equation)
   \mathbb{E}(x) &amp;amp;=&amp;amp; \frac{a}{a+b} \
   var(x) &amp;amp;=&amp;amp; \frac{ab}{(a+b)^2(a+b+1)}
 \end{eqnarray}
 贝塔分布是伯努利分布的共轭先验，经常用来表示二值事件的概率，其中$a,b$可以分别用来表示$X=0,X=1$时的先验数目。对上面列出的分布进行简单的总结：&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/b1.png", height=300pt, width=450pt&gt;&lt;/p&gt;
&lt;h3&gt;联合分布&lt;/h3&gt;
&lt;p&gt;前面的分布都是单变量的，下面看一下多变量随机变量的情况。对于多维随机变量，可以看作多个随机变量的组合。联合分布$p(x_1,x_2,...,x_D)$用来表示
这些随机变量之间的关系。就离散变量来说，多维随机变量的分布可以用一个多维的数组来表示，如果每一个随机变量有$K$个参数，则联合分布就有$K^D$个参数，可以用条
件独立关系来减少参数的数目。对于连续变量，可以限制密度函数的范围。&lt;/p&gt;
&lt;p&gt;与联合分布密切相关，经常用到的两类分布是：条件分布和边缘分布。条件分布可以用来对监督学习模型进行建模，监督学习问题，可以表示求解$p(t|\mathbf{x}
,\theta)$，其中，$\mathbf{x}$是特征向量，$t$是目标值，$\theta$是模型参数；边缘分布可以用来对非监督学习问题进行建模，非监督学习问
题，可以表示成求解$p(\mathbf{x}|\theta)$，很多时候，可以设置一些隐变量来表示那些未知的因素:
 \begin{eqnarray}
 % \nonumber to remove numbering (before each equation)
    p(t|\mathbf{x}, \theta) &amp;amp;=&amp;amp; \int_{\mathbf{z}}
p(t|\mathbf{x},\mathbf{z},\theta)\
    p(\mathbf{x}| \theta) &amp;amp;=&amp;amp; \int_{\mathbf{z}}p(\mathbf{x},\mathbf{z}|\theta)
 \end{eqnarray}
 \subsection{协方差和相关系数}
 协方差(covariance)和相关系数(correlation coefficients)可以用来表示两个随机变量之间的关系。协方差的定义如下：
 \begin{equation}\label{cov}
   cov(X,Y)=\mathbb{E}{[X-\mathbb{E}(X)][Y-\mathbb{E}(Y)]}
 \end{equation}
 相关系数定义为：
 \begin{equation}\label{cc}
   corr(X,Y)=\frac{cov(X,Y)}{\sqrt{var(X)}\sqrt{var(Y)}}
 \end{equation}
 注意上面式子中的方差均不能为零。&lt;/p&gt;
&lt;p&gt;对一个$D$维的随机变量$X=(x_1,x_2,...,x_D)$，可以用元素之间协方差的矩阵来表示两个$D$为随机变量之间的关系，矩阵如下：
 \begin{equation}\label{cov_matrix}
   cov(X)=\left(
            \begin{array}{cccc}
              var(x_1) &amp;amp; cov(x_1,x_2) &amp;amp; \cdots &amp;amp; cov(x_1,x_D) \\
              cov(x_2,x_1) &amp;amp; var(x_2) &amp;amp; \cdots &amp;amp; cov(x_2,x_D) \\
              \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
              cov(x_D,x_1) &amp;amp; cov(x_D,x_2) &amp;amp; \cdots &amp;amp; cov(x_D,x_D)
            \end{array}
          \right)
 \end{equation}
 该矩阵称为协方差矩阵。&lt;/p&gt;
&lt;h4&gt;多变量高斯分布(multivariate guassian distribution)&lt;/h4&gt;
&lt;p&gt;多变量高斯分布是对单变量高斯分布的扩展，也是在机器学习中用的最多的一类分布。&lt;/p&gt;
&lt;p&gt;\begin{equation}
  \mathcal N( \mathbf{x}|\mu,\Sigma)=\frac{1}{(2\pi)^{D/2}|\Sigma|^{1/2}}\exp\{
-(\mathbf{x}-\mu)^T\Sigma^{-1}(\mathbf{x}-\mu)\}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中，$\mu$是均值向量，$\Sigma=cov(x)$是$D\times D$协方差矩阵。&lt;/p&gt;
&lt;p&gt;高斯分布有两个局限：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;矩阵求逆，计算代价大。在协方差是一般矩阵时，多变量高斯分布密度函数的参数数目是：$D+D(D+1)/2$，参数规模随着特征维度平方增长，矩阵求逆花费的代价大
。因此通常会对协方差矩阵做一些限制，比如限制为对角阵(这时候参数就变成了$2D$)，还可以进一步限制对角线上的元素值相同(这时候参数就变成了$D+1$)，是单位
矩阵乘以一个常数。三种不同的情况，如图\ref{gcovfig} 所示。&lt;/li&gt;
&lt;li&gt;高斯是单峰的，只有一个最大值，对一些多峰数据拟合不好。这个确定可以通过引入隐变量来解决，具体会在后面详细描述。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;均值和方差为：
\begin{eqnarray}
% \nonumber to remove numbering (before each equation)
  \mathbb{E}(\mathbf{x}) &amp;amp;=&amp;amp; \mu \\
  cov(\mathbf{x}) &amp;amp;=&amp;amp; \Sigma
\end{eqnarray}&lt;/p&gt;
&lt;p&gt;下面看几个简单的多变量高斯分布的例子，从左到右，协方差矩阵分别为：一般矩阵，对角矩阵，单位矩阵。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;# mvn is not included in the released scipy library, but will be included in future. &lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;numpy.linalg&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;lstsq&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;slogdet&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;eig&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;mvn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Sigma&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;implement PDF for multivariate normal distribution.&lt;/span&gt;
&lt;span class="sd"&gt;       Parameters:&lt;/span&gt;
&lt;span class="sd"&gt;         - X: n * D matrix&lt;/span&gt;
&lt;span class="sd"&gt;         - mu: mean vector&lt;/span&gt;
&lt;span class="sd"&gt;         - Sigma: covariance matrix&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;Xm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;
    &lt;span class="n"&gt;logv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Xm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;lstsq&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Sigma&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Xm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;slogdet&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Sigma&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;logv&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c"&gt;# parameters&lt;/span&gt;
&lt;span class="n"&gt;mu&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;Sigma1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;1.5&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mf"&gt;1.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;eig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Sigma1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;Sigma2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Sigma1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;Sigma3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eye&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;Sigma_s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;Sigma1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Sigma2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Sigma3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="c"&gt;# data&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;x1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;meshgrid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;x1&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;y1&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;  
&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c"&gt;# plot&lt;/span&gt;
&lt;span class="n"&gt;titles&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ordinary&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;diagonal&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;spherical&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Sigma&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Sigma_s&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Sigma_s&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mvn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Sigma&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contour&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;titles&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="/images/b_12_0.png" /&gt;&lt;/p&gt;
&lt;h4&gt;多变量学生分布(multivariate student's-t distribution)&lt;/h4&gt;
&lt;p&gt;多变量学生分布的形式如下：&lt;/p&gt;
&lt;p&gt;\begin{equation}
  p(\mathbf{x}|\mu,\Lambda,\nu)=\frac{\Gamma(\nu/2+\frac{D}{2})}{\Gamma(\nu/2)}
  \left(\frac{\Lambda^{1/2}}{(\pi\nu)^{D/2}}\right)\left[1+\frac{(\mathbf{x}-
  \mathrm{\mu})^T \Lambda^{-1}(\mathbf{x}-\mathrm{\mu})}{\nu}\right]^{-\nu/2-1/2}
\end{equation}&lt;/p&gt;
&lt;p&gt;在$\nu\rightarrow \infty$时，学生分布就成了高斯分布。均值和方差分别为：&lt;/p&gt;
&lt;p&gt;\begin{eqnarray}
% \nonumber to remove numbering (before each equation)
   \mathbb{E}(\mathbf{x}) &amp;amp;=&amp;amp; \mu \\
  cov(\mathbf{x}) &amp;amp;=&amp;amp; \frac{\nu}{\nu-2}\Lambda
\end{eqnarray}&lt;/p&gt;
&lt;h4&gt;狄利克雷分布(dirichlet distribution)&lt;/h4&gt;
&lt;p&gt;狄利克雷分布是在贝塔分布的多变量的扩展，是类别分布和多项式分布的共轭先验，在贝叶斯估计中常用作先验。对于随机变量$X=(x_1,x_2,...,x_K)$，满足
下面的条件：&lt;/p&gt;
&lt;p&gt;\begin{equation}\label{dir_condition}
  0 \leq x_k \leq 1;
  \sum_k x_k =1.
\end{equation}&lt;/p&gt;
&lt;p&gt;其密度函数形式如下：&lt;/p&gt;
&lt;p&gt;\begin{equation}\label{dirich}
  p(\mathbf{x}|\alpha)= \frac{1}{B(\alpha)}\prod_{k=1}^{K} x_k ^{\alpha_k-1}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中,$\alpha=(\alpha_1,\alpha_2,...,\alpha_K)$，
\begin{equation}\label{dirb}
  B(\alpha)=\frac{\prod_{k=1}^{K}\Gamma(\alpha_k)}{\Gamma\left(\sum_{k=1}^K
  \alpha_k\right)}
\end{equation}&lt;/p&gt;
&lt;p&gt;\begin{eqnarray}
% \nonumber to remove numbering (before each equation)
  \mathbb{E}(\mathbf{x}) &amp;amp;=&amp;amp; \mu \\
  cov(\mathbf{x}) &amp;amp;=&amp;amp; \Sigma
\end{eqnarray}&lt;/p&gt;
&lt;h3&gt;怎么选择分布？&lt;/h3&gt;
&lt;p&gt;前面几节看了很多的分布形式，但是在解决一个具体问题的时候，我们收集了很多的数据，一个自然的疑问就是：用来解决问题的数据到底符合一个什么样的分布？因为只要我们知道
了我们数据的分布，我们就能根据未来的新数据的概率大小在做相应的决策。选择一个分布，可以分为两个阶段：首先，确定分布的密度函数是什么类型，是高斯分布还是伽马分布？
其次，对选择的密度函数，选择恰当的函数参数。&lt;/p&gt;
&lt;p&gt;对选择分布形式来说，这是一个比较困难的事情。简单的总结一下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;根据专家的经验来决定服从什么样的分布。这是凭人的经验来决定的，比如知道噪声可以用高斯来拟合，次品率可以用泊松分布；&lt;/li&gt;
&lt;li&gt;使用统计数字，比如均值，中位数，众数等的关系，来简单的决定分布的形式。比如：对于高斯这样的对称分布，均值和中位数应该相等；对于伽马这样的“左偏”的分布，均值
应该大于中位数。&lt;/li&gt;
&lt;li&gt;使用直方图和密度估计的方式做出样本数据的图像，看一下符合什么样的分布。当数据很大的时候，根据大数定理，可以很好的估计。&lt;/li&gt;
&lt;li&gt;使用统计测试的方式，比如卡方检验。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于如何决定模型中的参数，在第一章中有过描述，比如可以使用交叉验证的方法进行参数选择。&lt;/p&gt;
&lt;h3&gt;模型参数的估计方法&lt;/h3&gt;
&lt;p&gt;我们已经知道，监督学习问题和非监督学习问题分别对应着求解$p(t|\mathbf{x},\theta)$，$p(\mathbf{x}|\theta)$的概率分布
。本节讨论的是关于如何求解参数，来确定模型的分布。这里讨论三种比较常用的方法：最大似然估计方法(maximum likelihood
estimation,MLE)，最大后验估计方法(maximum posterior estimation,MAP)，贝叶斯估计(Bayesian
estimation)。&lt;/p&gt;
&lt;p&gt;设训练集(training set)：$ \mathbf{D}
=\{(\mathbf{x}_1,t_1),(\mathbf{x}_2,t_2),...,(\mathbf{x}_m,t_m)\}$,
每一行$(\mathbf{x}_i,t_i)$ 称为一个实例(example)，其中，$\mathbf{x}_i$通常称为特征向量(feature
vector),
$t_i$称为目标值。把$\mathbf{x}$的定义域记作$\mathcal{X}$，把$\mathbf{t}$的定义域记作$\mathcal{T}$。&lt;/p&gt;
&lt;h4&gt;最大似然估计&lt;/h4&gt;
&lt;p&gt;在最大似然估计中，将$\theta$看作单纯的参数。最大似然估计(MLE)寻找使得似然函数(likelihood function)最大的参数值，作为最优的参数
取值$\theta$。对于非监督问题来说，假设数据点$\mathbf{D}={\mathbf{x}_1,\mathbf{x}_2,...,\mathbf{x}
_m}$ 之间是独立同分布的(i.i.d)，则似然函数的定义如下：&lt;/p&gt;
&lt;p&gt;\begin{equation}
  \mathcal{L}(\theta) = p(\mathbf x_1,\mathbf x_2,...,\mathbf x_m|\theta)
=\prod_{i=1}^{m}p(\mathbf x_i|\theta)
\end{equation}&lt;/p&gt;
&lt;p&gt;在实际的应用中，经常使用的是对数似然函数如下：&lt;/p&gt;
&lt;p&gt;\begin{equation}
  \ln\mathcal{L}(\theta) = \sum_{i=1}^{m}\ln p(\mathbf{x}_i|\theta)
\end{equation}&lt;/p&gt;
&lt;p&gt;似然函数根据变量类型不同，有不一样的解释。对离散变量来说，似然的值就是在选定某些参数的情况下，数据点的概率；而对于连续变量来说，由于使用的是密度函数，则似然的值
没有任何物理意义，只是用来比较选定不同参数时彼此之间的大小。则最大似然估计就是求解下面的式子：&lt;/p&gt;
&lt;p&gt;\begin{equation}
  \theta^* = arg\max_{\theta} \ln\mathcal{L}(\theta)
\end{equation}&lt;/p&gt;
&lt;p&gt;值得注意的是似然函数的最大值不一定唯一，也不一定存在。最大似然估计有两个缺点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在数据点比较少的情况下，容易过拟合。&lt;/li&gt;
&lt;li&gt;方法的鲁棒性不好。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;上面的似然函数是对非监督问题来说的，对于监督学习，对数似然函数如下：&lt;/p&gt;
&lt;p&gt;\begin{equation}\label{sup-likelihood}
   \ln\mathcal{L}(\theta) = \sum_{i=1}^{m}\ln p(t_i|\mathbf{x}_i,\theta)
\end{equation}&lt;/p&gt;
&lt;h4&gt;最大后验估计&lt;/h4&gt;
&lt;p&gt;最大似然方法容易过拟合，为了减少过拟合，引入参数$\theta$的先验$p(\theta)$。由贝叶斯定理得：$p(\theta|\mathbf
x)=\frac{p(\mathbf x_1,\mathbf x_2,...,\mathbf
x_m|\theta)p(\theta)}{\int_{\theta}p(\mathbf x_1,\mathbf x_2,...,\mathbf
x_m|\theta)p(\theta)d\theta}$。 则最大后验估计就是求解下面的式子：&lt;/p&gt;
&lt;p&gt;\begin{eqnarray}
  \theta^* &amp;amp;=&amp;amp; arg\max_{\theta} \ln p(\theta|\mathbf x_1,\mathbf x_2,...,\mathbf
x_m) \
  \nonumber &amp;amp;=&amp;amp; arg\max_{\theta} \sum_{i=1}^{m}\ln p(\mathbf x_i|\theta) + \ln
p(\theta)
\end{eqnarray}&lt;/p&gt;
&lt;p&gt;它与最大似然估计的经典方法有密切关系，但是它使用了一个增大的优化目标，这种方法将被估计量的先验分布融合其中。最大后验估计可以看作是规则化（regularizat
ion）的最大似然估计。&lt;/p&gt;
&lt;p&gt;最大后验估计可以用以下几种方法计算：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;解析方法，当后验分布的最大值可以有一个解析解，比如使用共轭先验的情况下。&lt;/li&gt;
&lt;li&gt;通过如共扼梯度法或者牛顿法这样的数值优化方法进行，这通常需要一阶或者二阶导数，导数需要通过解析或者数值方法得到。&lt;/li&gt;
&lt;li&gt;通过期望最大化算法实现，这种方法不需要后验密度的导数。&lt;/li&gt;
&lt;li&gt;通过蒙特卡洛的方法求解。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;尽管使用了先验知识，但是MAP 通常不被认为是一种贝叶斯估计，因为它实际还是一种点估计，而贝叶斯使用估计量的分布来总结数据、得到推论。&lt;/p&gt;
&lt;p&gt;上面的式子是对非监督学习来说的，对于监督学习,&lt;/p&gt;
&lt;p&gt;$$p(\theta|\mathbf x,\mathbf t)=\frac{p(\mathbf t|\mathbf x_1,\mathbf
x_2,...,\mathbf x_m,\theta)p(\theta)}{\int_{\theta}p(\mathbf t,\mathbf
x_1,\mathbf x_2,...,\mathbf x_m,\theta)p(\theta)d\theta}$$&lt;/p&gt;
&lt;p&gt;所以MAP求解下面的公式：&lt;/p&gt;
&lt;p&gt;\begin{equation}\label{s-map}
   \theta^* = arg\max_{\theta} \sum_{i=1}^{m}\ln p(t_i|\mathbf x_i,\theta) + \ln
p(\theta)
\end{equation}&lt;/p&gt;
&lt;h4&gt;贝叶斯估计&lt;/h4&gt;
&lt;p&gt;贝叶斯估计与最大似然估计，最大后验估计不同，在MLE，MAP方法中，都是寻找一个最优的参数值$\theta^*$，而在贝叶斯估计中，直接求在给定数据的条件下，对
新数据的预测分布$p(\mathbf x|\mathbf D)$:&lt;/p&gt;
&lt;p&gt;\begin{equation}\label{bayesian}
  p(\mathbf x|\mathbf D) = \int_{\theta} p(\mathbf x|\theta)p(\theta|\mathbf
D)d\theta
\end{equation}&lt;/p&gt;
&lt;p&gt;对于监督学习，可以求解下面的式子：&lt;/p&gt;
&lt;p&gt;\begin{equation}\label{s-bayesian}
  p(t|\mathbf x,\mathbf D) = \int_{\theta} p(t|\mathbf x,\theta)p(\theta|\mathbf
D)d\theta
\end{equation}&lt;/p&gt;
&lt;p&gt;从上面的式子可以看出，不在是求解最优的$\theta^*$，而是求出$\theta^*$的后验概率，再使用后验对所有可能的$\theta^*$进行积分。随
之带来的是计算上的问题，如果积分不存在解析解，那只能采取一些近似算法来做处理，比如变分法和蒙特卡洛方法。&lt;/p&gt;
&lt;h3&gt;随机变量的转换&lt;/h3&gt;
&lt;p&gt;这里讨论的是对一个随机变量$X$施加一个转换$f$，则新的随机变量$Y=f(X)$的分布于原来的分布有什么联系？在后面的模型中，会用到相关的知识，这里做一下描述
，为了讨论的方便，这里只讨论单变量的情况，对于多变量的情况，只是简单的做一下说明。&lt;/p&gt;
&lt;p&gt;直观的对于随机变量$X$是离散的情况，由于转换$f$是一个确定性的操作，这样原来$X=x$的概率也就被转移到了$Y=f(x)$，所以可以得到公式：&lt;/p&gt;
&lt;p&gt;\begin{equation}\label{d-trans}
  p_Y(y)=\sum_{x \in {x|f(x)=y}} p_X(x)
\end{equation}
其中：$p_X$表示随机变量$X$服从的分布。&lt;/p&gt;
&lt;p&gt;对于连续性的变量，可以采用相同的思想，连续性的随机变量的概率是在随机变量附近极小区间的一个积分值。对于在$(x,x+\delta
x)$的概率也就被转移到了$(y,y+\delta y)$，所以$p_X(x)\delta x\approx p_Y(y)\delta y$，所以可以得到：&lt;/p&gt;
&lt;p&gt;\begin{equation}\label{c-trans}
  p_Y(y)=  p_X(x) \mid\frac{dx}{dy}\mid
\end{equation}&lt;/p&gt;
&lt;h3&gt;蒙特卡罗近似&lt;/h3&gt;
&lt;p&gt;有时候使用随机变量的分布函数$f$进行计算，会使得问题变得非常困难。这时候可以采用一些近似的方法，常用的一种近似方法是：从分布中进行采样，得到$m$个样本点
$x_1,x_2,x_3,...,x_m$，然后使用经验分布${f(x_i)}_{i=1}^m$来代替$f$。这样的近似方法称为蒙特卡洛近似。&lt;/p&gt;
&lt;p&gt;使用蒙特卡洛近似，对一个函数$f(x)$求期望，可以近似为：
\begin{equation}
  \mathbb{E}(f)=\frac{1}{N}\sum_{i=1}^{m}f(x_i)
\end{equation}&lt;/p&gt;
&lt;h3&gt;引用&lt;/h3&gt;
&lt;p&gt;[1] Grinstead and Snell's Introduction to Probability, Peter G. Doyle, 2006.&lt;/p&gt;
&lt;p&gt;[2] Machine Learning: a Probabilistic Perspective, Kevin Patrick Murphy, 2012&lt;/p&gt;
&lt;p&gt;[3] Scipy, &lt;a href="http://www.scipy.org/"&gt;http://www.scipy.org/&lt;/a&gt;&lt;/p&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</summary><category term="Machine Learning"></category></entry><entry><title>机器学习简介</title><link href="/posts/2014/04/ml_introduce/" rel="alternate"></link><updated>2014-04-20T10:20:00+08:00</updated><author><name>Webdancer</name></author><id>tag:,2014-04-20:posts/2014/04/ml_introduce/</id><summary type="html">&lt;h3&gt;什么是机器学习(machine learning)？&lt;/h3&gt;
&lt;p&gt;这本书我们讨论的是关于“机器学习”相关的知识，那么什么是机器学习？从目的的角度来说，机器学习主要的目的在于构建可以根据过往的经验自动学习以
提高自身性能的计算机系统，以及了解自动学习背后的规律。其中广泛引用的是Tom Mitchell 给出的定义：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;对于某类任务T和性能度量P，如果一个计算机程序在任务T上，以P度量的性能随着经验E而不断改善，那么我们称这个计算机程序在从经验E中学习。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;看以看出，该定义的描述中，涉及到三个基本方面：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;任务T。我们要处理的是一个什么样的任务，是无人车问题，语言识别，还是人脸识别;&lt;/li&gt;
&lt;li&gt;性能标准P。我们使用什么样的标准来说明我们设计的系统的好坏，这对我们后面设计学习的目标函数时候有指导作用，我们根据标准P来设计和调整模型，满足我们的任务要求。&lt;/li&gt;
&lt;li&gt;训练经验E。从那里获取训练经验可能对我们系统学习有直接的影响。
&lt;!-- PELICAN_END_SUMMARY --&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;简言之，机器学习主要是设计和分析使计算机能够“学习”的模型、算法，这些模型、算法能从经验数据集中发现规律，并根据规律对未知数据做预测。机器学习可以用于数据分析，
从中挖掘出有价值的东西，现在我们生活在一个“大数据”的时代，而机器学习的性质决定了它能很好的应用在大数据上，这也正是它流行的原因。在本书中，考虑的是基于&lt;strong&gt;模型
&lt;/strong&gt;的机器学习方法，不同的模型对应着不同的假设，反映了人们对于数据中所蕴含的模式的认识。一个典型的机器学习系统的流程如图：&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/1-1.png" /&gt;&lt;/p&gt;
&lt;p&gt;根据已有的经验数据，我们不断的训练假设的模型，当模型训练完成以后，可以用它来对未知的新数据做出判断。本书讨论的范围局限在对不同的模型的考虑，在应用机器学习模型到
不同的问题时，数据一般都需要进行相应的处理，比如预处理，特征提取等，数据的特征提取过程对于机器学习的性能有很大的影响，但是内容太广泛，技巧太多，只能涉及到其中的
很少一部分。&lt;/p&gt;
&lt;p&gt;一般来说，在我们对模型进行假设以后，机器学习过程可以分为两个阶段：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;训练(training)&lt;/strong&gt;，又称&lt;strong&gt;学习(learning)}&lt;/strong&gt;，在训练阶段，我们使用已有的经验数据来进行模型的训练，选择模型的参数，得到最终的模型
假设： $h(x)$ ，在该阶段使用的数据一般称为训练集；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;测试(test)&lt;/strong&gt;：在测试阶段，我们使用训练阶段获得的模型 $h(x)$
对新数据做出预测，并且根据评价标准对模型进行评价，在该阶段使用的数据一般称为测试集。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这两个阶段都很重要，但是相对而言，学习阶段的算法更加复杂，困难，是本书关注的重点。&lt;/p&gt;
&lt;p&gt;举一个简单的例子：鸢尾花有多个不同的品种，现在我们希望设计一个计算机程序来自动的鸢尾花进行品种的分类。Iris
数据集采集了3个品种的150株鸢尾花在花萼和花瓣的长、宽数据
, 如所示：
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Sepal length&lt;/th&gt;
        &lt;th&gt;Sepal width&lt;/th&gt;
        &lt;th&gt;Petal length&lt;/th&gt;
        &lt;th&gt;Petal width&lt;/th&gt;
        &lt;th&gt; target&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;4.9&lt;/td&gt;
        &lt;td&gt;3.0&lt;/td&gt;
        &lt;td&gt;1.4&lt;/td&gt;
        &lt;td&gt;0.2&lt;/td&gt;
        &lt;td&gt;0 &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;4.7&lt;/td&gt;
        &lt;td&gt;3.2&lt;/td&gt;
        &lt;td&gt;1.3&lt;/td&gt;
        &lt;td&gt;0.2&lt;/td&gt;
        &lt;td&gt;0 &lt;/td&gt;
    &lt;/tr&gt;
        &lt;tr&gt;
        &lt;td&gt;...&lt;/td&gt;
        &lt;td&gt;...&lt;/td&gt;
        &lt;td&gt;...&lt;/td&gt;
        &lt;td&gt;...&lt;/td&gt;
        &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;&lt;/p&gt;
&lt;/table&gt;

&lt;p&gt;将上面表格中的数据用符号表示，第1行数据的特征花萼长度，花萼宽度，花瓣长度，花瓣宽度记作
${x_{1,1},x_{1,2},x_{1,3},x_{1,4}}$ ，类别记作 $t_{1}$
，则类似的第$i$行数据的特征花萼长度，花萼宽度，花瓣长度，花瓣宽度记作${x_{i,1},x_{i,2},x_{i,3},\mathbf x_{i,4}
}$，类别记作$t_{i}$。采用机器学习模型来解决这个问题，我们通常会假设一个模型$h(\mathbf{x};\theta)$，然后使用我们收集的数据集
对模型进行训练，得到模型里面的参数 $\theta$ ，从而确定模型$h(\mathbf{x})$，用该模型对未来见到的新的鸢尾花进行品种的分类。&lt;/p&gt;
&lt;p&gt;上面表格所列的数据形式，是我们本书讨论的机器学习模型的输入形式，整个表格构成的数据集合为训练集(training dataset)。
其中每一行$(\mathbf x_i,t_i)$称为一个实例， $\mathbf x_{i}$ 通常称为特征向量，$t_{i}$
称为目标值。把$\mathbf x$ 的定义域记作 $\mathcal X$ ，把$\mathbf t$ 的定义域记作 $\mathcal T$。在本例中$\mathcal X = \mathbb R^4, \mathcal T={1,2,3}$ 。&lt;/p&gt;
&lt;p&gt;学习得到的模型在测试集上的预测能力称为&lt;strong&gt;泛化&lt;/strong&gt;。实际的应用中，输入变量的变化使得训练集只能是实际输入空间的一小部分，所以泛化是机器学习算法的一个核心目标。即
使在大数据时代，这个目标依然没有改变，原因有二：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在我们获得的大量数据中，数据可能是冗余的，很多是重复出现，很多在输入空间中的点，在训练集中依然没有包含（实际上就是我们常讲的长尾理论）；&lt;/li&gt;
&lt;li&gt;相对于数据的增长速度，现有的算法的能力还没有突破性的提升。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;因此，即使在大数据时代，模型的泛化能力依然是学习的核心目标。&lt;/p&gt;
&lt;h3&gt;机器学习分类&lt;/h3&gt;
&lt;p&gt;根据数据集中的目标值 $t$ 是否已知，我们可以将机器学习问题分为两类：监督学习(supervised learning)和非监督学习(unsupervised
learning)。如果目标值是已知的，则将该类学习称之为监督学习，反之，若目标值未知，则称之为非监督学习。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在监督学习中，训练集合(training set)中的实例(example)是特征向量(feature vector)和目标值（通常是人们手工标注的标签或其
他值）的二元组，由训练集合得到一个从特征向量到目标值的映射，然后能够对任何的有效输入值做出预测。对于监督学习而言，根据目标值的取值范围不同，又可以分为&lt;strong&gt;分类(
classification) 问题&lt;/strong&gt;和 &lt;strong&gt;回归(regression)问题&lt;/strong&gt;。如果目标值取值范围是不连续，离散的集合，则该类问题称为 分类问题；如果目标
值的取值是连续的，则该类问题成为回归问题。监督学习是一个良好定义的问题，因为我们有很好的度量方式来衡量学习的好坏(比如目标值与预测值之间的误差)；&lt;/li&gt;
&lt;li&gt;非监督学习的目标是发现“有趣的模式”，可能是数据点的群组，输入数据的分布或是将数据空间的一个投影。它不是一个很好定义的问题，通常很难找到一个好的方式来指导学
习的过程。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在上面所列的鸢尾花品种识别问题中，因为目标值是已知的，所以它是一个监督学习问题，而目标值的取值范围是一个离散的集合，所以它又是一个分类问题。&lt;/p&gt;
&lt;p&gt;需要注意的是机器学习算法不只局限在这两种类型，比如一类称为半间督学习(semi-supervised
learning)算法，在训练的时候可以同时利用一部分有标记的数据和无标记的数据同时进行学习；还有一类称为增强学习(reinforcement
learning)的算法，可以通过与环境交互，最大化回报函数来进行学习。本书中，只会涉及到一部分的增强学习的内容。&lt;/p&gt;
&lt;h4&gt;监督学习&lt;/h4&gt;
&lt;p&gt;在本节主要讨论监督学习问题，包括分类问题和回归问题；下一节讨论非监督学习。对机器学习的模型的性质，要解决的问题种类有一个大致的了解。&lt;/p&gt;
&lt;h5&gt;分类&lt;/h5&gt;
&lt;p&gt;在分类问题，对于数据集 $\mathbf{D}$ 中的每个实例，寻找一个函数 $h(\mathbf{x}; {\theta}):\mathbf{x}\in
\mathbb{R}^n \rightarrow t\in \mathbf{t}$ ，其中 $ {\theta}$ 称为模型的参数， $\mathbf{t}$
是所有目标值取值的离散的集合。分类的模型通常称为&lt;strong&gt;分类器(classifier)&lt;/strong&gt;。根据 $t,\mathbf{t}$
取值的不同，又可以将分类问题进行具体的划分。如果 $t$ 的取值是一个标量(只包含一个数)，则可以根据 $\mathbf{t}$  集合进行分类，若
$\mathbf{t}$  集合包含两个元素，则为二类(binary class) 问题，若 $\mathbf{t}$ 集合包含多个元素，则为多类(multi
class)问题; 如果 $t$ 为向量(包含多个数)，则该类问题成为多标记(multi label)问题。&lt;/p&gt;
&lt;p&gt;寻找一个函数可以理解为：在参数空间中搜索，寻找一个最优的参数，使得函数可以很好的来拟合数据，形式化如下：&lt;/p&gt;
&lt;p&gt;$$
   {\theta}^* = arg\max_{ {\theta}}\mathbf{J}(X; {\theta}) = arg\max_{
{\theta}}\sum_{i=1}^m \mathbf{L}(\mathbf{x}_i; {\theta})
$$&lt;/p&gt;
&lt;p&gt;其中:  $\mathbf{J}$ 为目标函数， $\mathbf{L}$ 为损失函数，例如常用的平方损失(squared loss),  $m$
为数据集中实例的数目。如果从概率角度理解，分类的目标是寻找再给出 $x, {\theta}$  的条件下 $t$  的分布 $p(t|x, {\theta})$
。通常而言，我们首先假设数据服从某类分布，比如高斯分布，数据点之间服从独立同分布(i.i.d)特性，然后使用统计学习的方式，进行参数的估计。形式化通常有两种形式
，如下：&lt;/p&gt;
&lt;p&gt;$$ {\theta}^* = arg\max_{ {\theta}} p(\mathbf{t}|\mathbf{X}, {\theta}) =
arg\max_{ {\theta}} \prod_{i=1}^m p(t_i|\mathbf{x}_i, {\theta})
$$&lt;/p&gt;
&lt;p&gt;这种求解方法，称为最大似然估计(Maximum Likelihood Estimate, MLE)；或是
 $$
   {\theta}^* = arg\max_{ {\theta}} p( {\theta}|\mathbf{X},\mathbf{t}) =
arg\max_{ {\theta}} \prod_{i=1}^m p( {\theta}|\mathbf{x}_i,t_i)
$$&lt;/p&gt;
&lt;p&gt;这种求解方法，称为最大后验估计(Maximum Posterior Estimate, MAP)，其中， $p$ 为数据服从的分布。&lt;/p&gt;
&lt;p&gt;分类问题又称为模式识别(pattern recognition)，在第一节中所举的鸢尾花品种识别就是一个典型的分类问题，这里涉及到三个类别，每个实例只能属于其中
一个类别，按照我们前面的论述，该问题属于多类问题。其处理流程大致如图所示:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/1-3.png", height=300pt, width=400pt/&gt;&lt;/p&gt;
&lt;p&gt;上面的流程也是大多数监督学习算法的流程，在其中最重要，也是最困难的的两个地方就是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;建模。选择合适的模型，设计好要优化的目标函数。有时候可能问题的定义导致不能显式的写出一个优化目标函数。&lt;/li&gt;
&lt;li&gt;学习。设计有效的学习算法进行模型的学习，是我们在学习现有的模型中遇到的困难的地方，大多数的学习算法都有很多细节需要处理，但是在讨论的时候，需要忽略掉一些细节
，进行抽象。&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;回归&lt;/h5&gt;
&lt;p&gt;回归问题也属于监督学习问题，与分类问题不同，在回归问题中目标值是连续的，目标值一般都是有意义的，比如在预测明天的气温，如果明天的实际气温是12度，那么预测的是1
5度与18 度差别是很大的。这与分类问题是有很大差别的，比如在字符识别中，要识别5，我们识别成6还是7，都是一样的结果，识别都是错误的。
回归问题的定义如下，对于数据集 $\mathbf{D}$ 中的每个实例，寻找一个函数 $h(\mathbf{x};
{\theta}):\mathbf{x}\in \mathbb{R}^n \rightarrow t\in \mathbb{R}$，其中$ {\theta}$
称为模型的参数。按照输入变量的个数，如果输入变量是一个变量，则成为一元回归，否则，输入变量有多个变量，称为多元回归。从概率角度理解，回归问题与分类问题类似，寻找
再给出 $\mathbf{x}, {\theta}$  的条件下 $t$  的分布 $p(t|\mathbf{x}, {\theta})$ ，两者主要的区别是
$t$ 服从的分布是连续分布，而不再是离散分布。&lt;/p&gt;
&lt;p&gt;曲线拟合中，需要用观测到的实值变量来预测实值的目标变量，是一个典型的回归问题。回归问题中常用的误差函数(error function)是平方误差(squared
loss)。误差函数描述训练集中模型的预测值与实际值的差距。如下:&lt;/p&gt;
&lt;p&gt;$$\mathbf J (\theta)=\frac{1}{2}\sum_{i=1}^{m}{y(\mathbf
x_{i},\theta)-t_{i}}^{2}$$&lt;/p&gt;
&lt;p&gt;求解的过程一般就是对函数求导，得到关于 $\theta$ 的线性函数，从而可以得到一个解析解 $ \theta^*$ 。详细的过程会在后面章节给出。&lt;/p&gt;
&lt;p&gt;在现实世界中，回归问题有很多应用：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在商业领域，可以用来预测预测股价，投资风险等；&lt;/li&gt;
&lt;li&gt;在互联网领域，可以预测网站的排名，用户的视频浏览量等；&lt;/li&gt;
&lt;li&gt;在机器人领域，可以用来预测机器人手臂的位置；&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;非监督学习&lt;/h4&gt;
&lt;p&gt;非监督学习问题与监督学习问题有很大的不同，主要体现在在非监督学习中目标变量没有给出，使得非监督学习不是良好定义(ill-
defined)问题。根据不同的问题，我们可以有不同的定义，从而有不同的模型。从概率角度来说，非监督学习主要是寻找数据的分布$p(x|
{\theta})$，其中$ {\theta}$为分布的参数。按照统计的术语来说，是密度估计。从概率方面来理解，主要有两个不同：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;监督学习是寻找给出 $x, {\theta}$ 的条件下 $t$  的分布 $p(t|x, {\theta})$
，求解的是一个条件分布；而非监督学习寻找数据的分布 $p(x| {\theta})$ ；&lt;/li&gt;
&lt;li&gt;通常来说，监督学习中的 $t$ 是单变量的，因此求解的是单变量的分布；而非监督学习中求解一个多变量的分布。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;非监督学习相比于监督学习来说，一个明显的优势是模型不需要人手动的给数据添加标记，现在的数据越来越大，标记数据是一个既耗时费力，又花费比较大。本书中会涉及到很多非
监督学习的内容，比如聚类，PCA，隐变量发现等。有人认为非监督学习更符合人学习的本质，著名的机器学习专家Hinton认为：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;当我们学习看这个世界时，我们并没有被告诉正确的结果，我们只是在看。即使你的母亲告诉“那是一条狗”，但是这本身的信息很少。按照这种方式，一秒钟得到1比特的信息已
经很幸运了。人脑的视觉系统有 $10^{14}$ 的神经连接，我们人类只有 $10^9$ 秒寿命。所以你 $1$ 秒学习 $1$ 比特的信息是无用的，至少要
$1$ 秒学习 $10^5$ 比特的信息。而且你只能从输入本身得到这么多的信息，从其他地方根本无法得到。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;根据上面的论述，可以用下面的框图3表示机器学习的分类：&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/1-2.png"/&gt;&lt;/p&gt;
&lt;p&gt;这只是一个简单的框架，它省略了很多机器学习的重要部分，比如按照分类器分类能力的强弱会有强弱分类器，可以集成多个弱分类器进行集成学习(ensemble
learning)，可以使用标记数据和没有标记数据的半间督学习，对于多标记的输出，我们可以考虑输出的结构(structure prediction)等。&lt;/p&gt;
&lt;h3&gt;数据&lt;/h3&gt;
&lt;p&gt;现在是数据爆炸的“大数据时代”，人类在各个领域积累了很多的数据，尤其是互联网上的数据，怎么从这些数据中挖掘有价值的东西？可以使用什么样的工具？这都是摆在面前的有
意义的问题。可以说，机器学习是用来进行数据分析，从中挖掘有意义和价值的模式的最好的工具。对数据的形式有一个大致的了解，是设计、应用机器学习的基础。对数据更好的认
识，有利于设计更好的学习算法。此外，如果对数据有了比较好的认识，可以借鉴处理相似数据的时候使用的模型，从而快速的进行问题的解决。&lt;/p&gt;
&lt;p&gt;如何采集数据，如何存储，不是本书的论述范围。讨论模型的时候，数据已经做了抽象，形成了高级的数据对象，这里只是简单的进行说明。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;向量(vector)：这是表示训练集实例最常用的方式，也是本书中描述实例最常用到的表示。将现实世界中的对象抽象出各种属性(可能不是显式的)，然后用向量进行表
示对象是最常用的方式，比如在描述文档时候，可以将其组织成一个词汇表中出现单词的向量，某个词汇出现，该位置为1，否则为0；&lt;/li&gt;
&lt;li&gt;列表(list)：列表与向量不同的是不同实例的可以有不同数目的特征；&lt;/li&gt;
&lt;li&gt;集合(set)：用集合来表示实例在多实例学习中是常用的(multi-instance learning)。通常是实例包含很多潜在的因素，而这些潜在因素的影响
并不明确。例如在确定分子在某种新药中的有效性时，由于同一个分子有很多变种，不能很准确的确定每个变种的有效性（正例还是负例），就将一个分子表示成一个集合（包），里
面包含不同的分子变种；&lt;/li&gt;
&lt;li&gt;矩阵(matrices)：用矩阵可以来表示这个对象之间(pairwise )的关系,比较常用的矩阵有：相似度矩阵，用在协同过滤中的用户-产品矩阵等；&lt;/li&gt;
&lt;li&gt;树或是图：实例构成树或是图的节点，然后依此来做推断；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;使用不同的数据表示，可能决定后面使用的机器学习算法，从而对问题解决有一个比较重要的影响。&lt;/p&gt;
&lt;h3&gt;参数化，非参数化模型&lt;/h3&gt;
&lt;p&gt;由前面的内容知道，从概率角度讲，机器学习求解$p(t|x)$（监督学习）或是$p(x)$（非监督学习）。定义这样的模型有很多方式，其中一种方法是：模型中含有的参
数是固定，还是不固定，随着数据集增长而增长。参数固定的模型称为参数化模型，而参数不固定的模型称为非参数化模型。参数化估计的优点是求解相对简单，缺点是对数据的分布
做了太强的假设，可能模型分布与数据分布之间的差别较大。非参数化模型相比参数化模型来说更灵活，但是对于大数据来说，模型求解却更加复杂。&lt;/p&gt;
&lt;h4&gt;参数化模型&lt;/h4&gt;
&lt;p&gt;前面提到的单变量多项式拟合就是一个比较简单的参数化模型，假设目标值与输入之间符合下面的式子，
 $$ t = \theta_{0}+ \theta_{1}x +  \theta_{2}x^2 +... + \theta_{n}x^n
+\epsilon$$&lt;/p&gt;
&lt;p&gt;其中， $\epsilon$ 称为残差(residual),假设符合符合高斯分布 $\mathcal N(\epsilon|0,\beta)$
。那么目标变量$t$的分布函数为：
 $$p(y|X,\theta,\beta)=\mathcal N(t|\theta_{0}+ \theta_{1}x +... +
\theta_{n}x^n,\beta)$$&lt;/p&gt;
&lt;p&gt;根据第二节中讨论的参数估计(MLE或是MAP)方式，我们可以求出 $ {\theta}$ 。&lt;/p&gt;
&lt;h4&gt;非参数化模型&lt;/h4&gt;
&lt;p&gt;在这里举一个K-近邻的方法来说明非参数话估计。简单的说，K-近邻方法就是用某个样本周围最邻近的K个样本的标记来决定该样本的标记。这种学习方式称为基于实例的学习
(instance-based learning)或是基于记忆的学习(memory-based learning)。概率分布可以形式化如下:&lt;/p&gt;
&lt;p&gt;$$p(t|\mathbf{x},D,K) = \frac{1}{K} \sum_{i\in NK(\mathbf{x})} \mathbb{I}
(\mathbf{x}_i==t)$$&lt;/p&gt;
&lt;p&gt;其中， $NK(x)$ 表示 $x$ 最近的 $K$ 个点的索引， $\mathbb{I}$
为&lt;a href="http://zh.wikipedia.org/zh/%E6%8C%87%E7%A4%BA%E5%87%BD%E6%95%B0"&gt;指示函数&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;此外，最近贝叶斯参数估计受到了越来越多的注意，后面的部分会对相关的内容进行讨论。&lt;/p&gt;
&lt;h3&gt;过拟合(Overfitting)&lt;/h3&gt;
&lt;p&gt;再来看一下多项式拟合的例子，图7中用20次多项式拟合21个数据点时候，函数穿过了所有的数据点，在训练集上的错误均值(mean square erro,MSE)为
0。当有新的实例到来时候，这样的函数预测性能很差，也就是模型的泛化能力不够。这种现象称为过拟合。这是由于训练数据集只能占到数据总体的一部分，在训练数据上完全拟合
，可能不能反映数据模型的真实分布，从而造成泛化能力很差。解决过拟合问题，提高模型的泛化能力，是机器学习的核心目标，通常的方法有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;降低特征数目；&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;人为的选择部分特征，忽略掉其他的特征；&lt;/li&gt;
&lt;li&gt;使用模型选择算法，进行特征选择。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;使用正则化(regularization)方法；&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;保留所有的特征，减小参数的数值；&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;模型选择&lt;/h3&gt;
&lt;p&gt;在第一节中已经说过，机器学习分为训练阶段和测试阶段，我们可以用某种标准来衡量模型，比如在回归问题中，用错误均值(MSE)。在训练阶段，我们可以得出一个训练错误(
training error )，但是我们更关注的是测试阶段的泛化错误(generalized error)。 从多项式拟合的例子可以看到,用1次多项式拟合的时
候，模型都不能很好的拟合训练数据集中的点，训练错误高，泛化错误高，这种现象称为欠拟合(underfits)。当用20次多项式拟合的时候，出现了过拟合的现象。在实
际的应用中，我们需要选择恰当的模型，使得模型能有比较好的泛化能力。&lt;/p&gt;
&lt;p&gt;但是在实际中，测试集是未知的，在模型训练和选择阶段是不可见的，只能用来评估模型的准确性。在实际中，通常会选择将训练集分为两部分：训练集(training
set)和验证集(validation set)，用训练集训练模型，然后使用验证集来验证模型，选在在验证集上表现最后的模型作为最终的模型，选定模型后，再在整个训
练集(包含验证集)上重新训练。通常，80%作为训练集，20%作为测试集。最后使用测试集对模型评估。&lt;/p&gt;
&lt;p&gt;实际应用中，数据集的规模是有限的，使用上面的方式会浪费很多的数据，验证集过小，给出的估计的噪声又很大，所以人们提出了一种简单有效的方式-交叉验证(cross
validation)用来进行模型的选择。交叉验证的方式很简单：将训练集数据分为K-折(fold)（常用的方式就是均分），其中K-1折的数据用来训练模型，剩下的
用来做验证集评估模型，重复进行K次，每一折都被当作过验证集，最后把K次的结果平均，用来作为这个模型的评估。如果K=m，则成为留一验证(leave-one-
out,LOOCV)。交叉验证有如下的缺点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;训练的次数随着K而增长，这对那些训练一个模型就需要很长时间的问题来说，就需要更多的时间&lt;/li&gt;
&lt;li&gt;如果模型中有多个参数需要选择，那么这些参数的组合可能导致训练次数呈指数增长；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;归根到底就是，在大数据情况下，模型训练本省就非常耗时，而交叉验证耗时更多，使得最终的程序没有良好的伸缩性(scalable)。&lt;/p&gt;
&lt;h3&gt;维度灾难(Curse of dimensionality)&lt;/h3&gt;
&lt;p&gt;我们这里看一个例子(在PRML和MLAPP里面都提到)。比如数据分布在 $D$ 维的单位超立方体中，现在我们要用KNN方法来进行分类，假设对某一个测试点 $x$
，要满足训练集中该点周围的点的比例要占到数据集的 $f$ 分之一。则我们可以以 $x$ 为中心，做一个超立方体，则该立方体的边长则至少要满足：
$e=f^{\frac{1}{D}}$ 。比如，要包含十分之一的数据，即 $f=10$ ，当 $D=1$  时， $e_1=0.1$ ，但是当 $D=10$
时， $e_{10}=0.79$
，这样数据就看起来不再是“局部的”，那么模型估计可能就不是很好。这种随着空间维度增加而出现的问题，称为维度灾难。下图表示了随着$e$变化$f$的变化情况。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;e&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;d_s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;d_s&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;d=&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;e&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Volumn fraction&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="/images/a_1_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;从图中可以看出：即使在比例比较小的情况下，当维度很高的时候，超立方体的边长都很大，这样只有小部分数据点离测试点较近，大多数数据点都在离要测试的数据较远的一个狭长
区域。这也就说明在高维空间中，训练数据会变得比较稀疏，为了支持需要的结果，数据集可能需要指数型的增长，这样在低维空间中的直观想法，可能由于维度灾难的出现，而不能
直接在高维空间中直接的使用。&lt;/p&gt;
&lt;p&gt;在具体的机器学习问题中，虽然维度灾难有很大的影响，但是这并不妨碍我们寻找在高维空间中有效的方法，原因有二:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;高维空间中特征冗余，真正的数据存在于一个低纬度的空间，特别需要指出的是目标值的重要的变化方向可能很局限(PCA的原理)，这样可以采用一些降维的方法进行处理；&lt;/li&gt;
&lt;li&gt;真实的数据满足一些光滑的特性(可能是局部性)，这样输入值的变化就会导致目标值的相应变化，这样可以采用一些类似插值的技巧进行处理；&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;工具&lt;/h3&gt;
&lt;p&gt;本书以Python语言来实现算法，用到Scikit-learn软件包等第三方机器学习相关的软件。使用IPython
Notebook可以很容易的来进行学习。下面看几个简单的例子。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;sklearn.datasets&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;datasets&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pprint&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;pprint&lt;/span&gt;
&lt;span class="n"&gt;dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;datasets&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load_iris&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;The first 5 feature vectors in dataset:&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;pprint&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;The first 5 target values in dataset:&amp;#39;&lt;/span&gt; 
&lt;span class="n"&gt;pprint&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;下面看一下用Scikit-learn对上面提到的鸢尾花进行品种的分类，代码非常简洁。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;linear_model&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;lm&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.cross_validation&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.metrics&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;classification_report&lt;/span&gt;
&lt;span class="c"&gt;# split the iris dataset into training dataset and test dataset&lt;/span&gt;
&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
&lt;span class="c"&gt;# get the logistic regression classifier  &lt;/span&gt;
&lt;span class="n"&gt;lr_clf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LogisticRegression&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;lr_clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c"&gt;# predict the label for test dataset&lt;/span&gt;
&lt;span class="n"&gt;y_pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lr_clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c"&gt;# report the evalutation&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Detail classification report:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;classification_report&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;Detail&lt;/span&gt; &lt;span class="n"&gt;classification&lt;/span&gt; &lt;span class="n"&gt;report&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
             &lt;span class="n"&gt;precision&lt;/span&gt;    &lt;span class="n"&gt;recall&lt;/span&gt;  &lt;span class="n"&gt;f1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt;   &lt;span class="n"&gt;support&lt;/span&gt;

          &lt;span class="mi"&gt;0&lt;/span&gt;       &lt;span class="mf"&gt;1.00&lt;/span&gt;      &lt;span class="mf"&gt;1.00&lt;/span&gt;      &lt;span class="mf"&gt;1.00&lt;/span&gt;        &lt;span class="mi"&gt;11&lt;/span&gt;
          &lt;span class="mi"&gt;1&lt;/span&gt;       &lt;span class="mf"&gt;1.00&lt;/span&gt;      &lt;span class="mf"&gt;0.62&lt;/span&gt;      &lt;span class="mf"&gt;0.76&lt;/span&gt;        &lt;span class="mi"&gt;13&lt;/span&gt;
          &lt;span class="mi"&gt;2&lt;/span&gt;       &lt;span class="mf"&gt;0.55&lt;/span&gt;      &lt;span class="mf"&gt;1.00&lt;/span&gt;      &lt;span class="mf"&gt;0.71&lt;/span&gt;         &lt;span class="mi"&gt;6&lt;/span&gt;

&lt;span class="n"&gt;avg&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;total&lt;/span&gt;       &lt;span class="mf"&gt;0.91&lt;/span&gt;      &lt;span class="mf"&gt;0.83&lt;/span&gt;      &lt;span class="mf"&gt;0.84&lt;/span&gt;        &lt;span class="mi"&gt;30&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;下面看一下如何来进行模型选择，进行模型选择的方式有比较多，在Scikit-learn中cross_validation模块就有K-
flod，LOO，LPO等。下面的例子中，使用网格法来确定模型的超参数，使用最常用的K-flod来进行模型的选择。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;grid_search&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.metrics&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;classification_report&lt;/span&gt;
&lt;span class="n"&gt;parameters&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;C&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:[&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;]}&lt;/span&gt;
&lt;span class="n"&gt;clf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;grid_search&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GridSearchCV&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lr_clf&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Best parameters found on train set:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_estimator_&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mean_score&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scores&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grid_scores_&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="si"&gt;%0.3f&lt;/span&gt;&lt;span class="s"&gt; (+/- &lt;/span&gt;&lt;span class="si"&gt;%0.03f&lt;/span&gt;&lt;span class="s"&gt;) for &lt;/span&gt;&lt;span class="si"&gt;%r&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mean_score&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; 
&lt;span class="n"&gt;y_true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Detail classification report:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;classification_report&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;通过上面的例子可以看到，通过使用交叉验证我们得到了一个更好的模型，来对测试集中的数据进行预测。&lt;/p&gt;
&lt;h3&gt;引用&lt;/h3&gt;
&lt;p&gt;[1] 机器学习, Tom M. Mitchell, 曾华军等译，机械工业出版社, 2003.&lt;/p&gt;
&lt;p&gt;[2] Machine Learning: a Probabilistic Perspective, Kevin Patrick Murphy, 2012&lt;/p&gt;
&lt;p&gt;[3] Pattern Recognition and Machine Learning, Christopher M. Bishop, 2006&lt;/p&gt;
&lt;p&gt;[4] Scikit-learn: &lt;a href="http://scikit-
learn.org/stable/index.html"&gt;http://scikit-learn.org/stable/index.html&lt;/a&gt;&lt;/p&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</summary><category term="Machine Learning"></category></entry></feed>