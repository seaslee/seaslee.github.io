<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>AI's bazaar</title><link href="/" rel="alternate"></link><link href="/feeds/machine-learning.atom.xml" rel="self"></link><id>/</id><updated>2014-04-20T10:20:00+08:00</updated><entry><title>机器学习简介</title><link href="/posts/2014/04/ml_introduce/" rel="alternate"></link><updated>2014-04-20T10:20:00+08:00</updated><author><name>Webdancer</name></author><id>tag:,2014-04-20:posts/2014/04/ml_introduce/</id><summary type="html">&lt;h3&gt;什么是机器学习(machine learning)？&lt;/h3&gt;
&lt;p&gt;这本书我们讨论的是关于“机器学习”相关的知识，那么什么是机器学习？回答这个问题可以从它的目的角度来回答，机器学习主要的目的在于构建可以根据过往的经验自动学习以
提高自身性能的计算机系统，以及了解自动学习背后的规律。其中广泛引用的是Tom Mitchell 给出的定义：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;对于某类任务T和性能度量P，如果一个计算机程序在任务T上，以P度量的性能随着经验E而不断改善，那么我们称这个计算机程序在从经验E中学习。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;看以看出，该定义的描述中，涉及到三个基本方面：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;任务T。我们要处理的是一个什么样的任务，是无人车问题，语言识别，还是人脸识别;&lt;/li&gt;
&lt;li&gt;性能标准P。我们使用什么样的标准来说明我们设计的系统的好坏，这对我们后面设计学习的目标函数时候有指导作用，我们根据标准P来设计和调整模型，满足我们的任务要求。&lt;/li&gt;
&lt;li&gt;训练经验E。从那里获取训练经验可能对我们系统学习有直接的影响。
&lt;!-- PELICAN_END_SUMMARY --&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;简言之，机器学习主要是设计和分析使计算机能够“学习”的模型、算法，这些模型、算法能从经验数据集中发现规律，并根据规律对未知数据做预测。机器学习可以用于数据分析，
从中挖掘出有价值的东西，现在我们生活在一个“大数据”的时代，而机器学习的性质决定了它能很好的应用在大数据上，这也正是它流行的原因。在本书中，考虑的是基于&lt;strong&gt;模型
&lt;/strong&gt;的机器学习方法，不同的模型对应着不同的假设，反映了人们对于数据中所蕴含的模式的认识。一个典型的机器学习系统的流程如图：&lt;/p&gt;
&lt;p&gt;&lt;img src="images/1-1.png" /&gt;&lt;/p&gt;
&lt;p&gt;根据已有的经验数据，我们不断的训练假设的模型，当模型训练完成以后，可以用它来对未知的新数据做出判断。本书讨论的范围局限在对不同的模型的考虑，在应用机器学习模型到
不同的问题时，数据一般都需要进行相应的处理，比如预处理，特征提取等，数据的特征提取过程对于机器学习的性能有很大的影响，但是内容太广泛，技巧太多，只能涉及到其中的
很少一部分。&lt;/p&gt;
&lt;p&gt;一般来说，在我们对模型进行假设以后，机器学习过程可以分为两个阶段：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;训练(training)&lt;/strong&gt;，又称&lt;strong&gt;学习(learning)}&lt;/strong&gt;，在训练阶段，我们使用已有的经验数据来进行模型的训练，选择模型的参数，得到最终的模型
假设： $h(x)$ ，在该阶段使用的数据一般称为训练集；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;测试(test)&lt;/strong&gt;：在测试阶段，我们使用训练阶段获得的模型 $h(x)$
对新数据做出预测，并且根据评价标准对模型进行评价，在该阶段使用的数据一般称为测试集。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这两个阶段都很重要，但是相对而言，学习阶段的算法更加复杂，困难，是本书关注的重点。&lt;/p&gt;
&lt;p&gt;举一个简单的例子：鸢尾花有多个不同的品种，现在我们希望设计一个计算机程序来自动的鸢尾花进行品种的分类。Iris
数据集采集了3个品种的150株鸢尾花在花萼和花瓣的长、宽数据
, 如所示：
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Sepal length&lt;/th&gt;
        &lt;th&gt;Sepal width&lt;/th&gt;
        &lt;th&gt;Petal length&lt;/th&gt;
        &lt;th&gt;Petal width&lt;/th&gt;
        &lt;th&gt; target&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;4.9&lt;/td&gt;
        &lt;td&gt;3.0&lt;/td&gt;
        &lt;td&gt;1.4&lt;/td&gt;
        &lt;td&gt;0.2&lt;/td&gt;
        &lt;td&gt;0 &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;4.7&lt;/td&gt;
        &lt;td&gt;3.2&lt;/td&gt;
        &lt;td&gt;1.3&lt;/td&gt;
        &lt;td&gt;0.2&lt;/td&gt;
        &lt;td&gt;0 &lt;/td&gt;
    &lt;/tr&gt;
        &lt;tr&gt;
        &lt;td&gt;...&lt;/td&gt;
        &lt;td&gt;...&lt;/td&gt;
        &lt;td&gt;...&lt;/td&gt;
        &lt;td&gt;...&lt;/td&gt;
        &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;&lt;/p&gt;
&lt;/table&gt;

&lt;p&gt;将上面表格中的数据用符号表示，第1行数据的特征花萼长度，花萼宽度，花瓣长度，花瓣宽度记作
${x_{1,1},x_{1,2},x_{1,3},x_{1,4}}$ ，类别记作 $t_{1}$
，则类似的第$i$行数据的特征花萼长度，花萼宽度，花瓣长度，花瓣宽度记作${x_{i,1},x_{i,2},x_{i,3},\mathbf x_{i,4}
}$，类别记作$t_{i}$。采用机器学习模型来解决这个问题，我们通常会假设一个模型$h(\mathbf{x};\theta)$，然后使用我们收集的数据集
对模型进行训练，得到模型里面的参数 $\theta$ ，从而确定模型$h(\mathbf{x})$，用该模型对未来见到的新的鸢尾花进行品种的分类。&lt;/p&gt;
&lt;p&gt;上面表格所列的数据形式，是我们本书讨论的机器学习模型的输入形式，整个表格构成的数据集合为训练集(training dataset)。
其中每一行$(\mathbf x_i,t_i)$称为一个实例， $\mathbf x_{i}$ 通常称为特征向量，$t_{i}$
称为目标值。把$\mathbf x$ 的定义域记作 $\mathcal X$ ，把$\mathbf t$ 的定义域记作 $\mathcal T$。在本例中$\mathcal X = \mathbb R^4, \mathcal T={1,2,3}$ 。&lt;/p&gt;
&lt;p&gt;学习得到的模型在测试集上的预测能力称为&lt;strong&gt;泛化&lt;/strong&gt;。实际的应用中，输入变量的变化使得训练集只能是实际输入空间的一小部分，所以泛化是机器学习算法的一个核心目标。即
使在大数据时代，这个目标依然没有改变，原因有二：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在我们获得的大量数据中，数据可能是冗余的，很多是重复出现，很多在输入空间中的点，在训练集中依然没有包含（实际上就是我们常讲的长尾理论）；&lt;/li&gt;
&lt;li&gt;相对于数据的增长速度，现有的算法的能力还没有突破性的提升。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;因此，即使在大数据时代，模型的泛化能力依然是学习的核心目标。&lt;/p&gt;
&lt;h3&gt;机器学习分类&lt;/h3&gt;
&lt;p&gt;根据数据集中的目标值 $t$ 是否已知，我们可以将机器学习问题分为两类：监督学习(supervised learning)和非监督学习(unsupervised
learning)。如果目标值是已知的，则将该类学习称之为监督学习，反之，若目标值未知，则称之为非监督学习。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在监督学习中，训练集合(training set)中的实例(example)是特征向量(feature vector)和目标值（通常是人们手工标注的标签或其
他值）的二元组，由训练集合得到一个从特征向量到目标值的映射，然后能够对任何的有效输入值做出预测。对于监督学习而言，根据目标值的取值范围不同，又可以分为&lt;strong&gt;分类(
classification) 问题&lt;/strong&gt;和 &lt;strong&gt;回归(regression)问题&lt;/strong&gt;。如果目标值取值范围是不连续，离散的集合，则该类问题称为 分类问题；如果目标
值的取值是连续的，则该类问题成为回归问题。监督学习是一个良好定义的问题，因为我们有很好的度量方式来衡量学习的好坏(比如目标值与预测值之间的误差)；&lt;/li&gt;
&lt;li&gt;非监督学习的目标是发现“有趣的模式”，可能是数据点的群组，输入数据的分布或是将数据空间的一个投影。它不是一个很好定义的问题，通常很难找到一个好的方式来指导学
习的过程。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在上面所列的鸢尾花品种识别问题中，因为目标值是已知的，所以它是一个监督学习问题，而目标值的取值范围是一个离散的集合，所以它又是一个分类问题。&lt;/p&gt;
&lt;p&gt;需要注意的是机器学习算法不只局限在这两种类型，比如一类称为半间督学习(semi-supervised
learning)算法，在训练的时候可以同时利用一部分有标记的数据和无标记的数据同时进行学习；还有一类称为增强学习(reinforcement
learning)的算法，可以通过与环境交互，最大化回报函数来进行学习。本书中，只会涉及到一部分的增强学习的内容。&lt;/p&gt;
&lt;h4&gt;监督学习&lt;/h4&gt;
&lt;p&gt;在本节主要讨论监督学习问题，包括分类问题和回归问题；下一节讨论非监督学习。对机器学习的模型的性质，要解决的问题种类有一个大致的了解。&lt;/p&gt;
&lt;h5&gt;分类&lt;/h5&gt;
&lt;p&gt;在分类问题，对于数据集 $\mathbf{D}$ 中的每个实例，寻找一个函数 $h(\mathbf{x}; {\theta}):\mathbf{x}\in
\mathbb{R}^n \rightarrow t\in \mathbf{t}$ ，其中 $ {\theta}$ 称为模型的参数， $\mathbf{t}$
是所有目标值取值的离散的集合。分类的模型通常称为&lt;strong&gt;分类器(classifier)&lt;/strong&gt;。根据 $t,\mathbf{t}$
取值的不同，又可以将分类问题进行具体的划分。如果 $t$ 的取值是一个标量(只包含一个数)，则可以根据 $\mathbf{t}$  集合进行分类，若
$\mathbf{t}$  集合包含两个元素，则为二类(binary class) 问题，若 $\mathbf{t}$ 集合包含多个元素，则为多类(multi
class)问题; 如果 $t$ 为向量(包含多个数)，则该类问题成为多标记(multi label)问题。&lt;/p&gt;
&lt;p&gt;寻找一个函数可以理解为：在参数空间中搜索，寻找一个最优的参数，使得函数可以很好的来拟合数据，形式化如下：&lt;/p&gt;
&lt;p&gt;$$
   {\theta}^* = arg\max_{ {\theta}}\mathbf{J}(X; {\theta}) = arg\max_{
{\theta}}\sum_{i=1}^m \mathbf{L}(\mathbf{x}_i; {\theta})
$$&lt;/p&gt;
&lt;p&gt;其中:  $\mathbf{J}$ 为目标函数， $\mathbf{L}$ 为损失函数，例如常用的平方损失(squared loss),  $m$
为数据集中实例的数目。如果从概率角度理解，分类的目标是寻找再给出 $x, {\theta}$  的条件下 $t$  的分布 $p(t|x, {\theta})$
。通常而言，我们首先假设数据服从某类分布，比如高斯分布，数据点之间服从独立同分布(i.i.d)特性，然后使用统计学习的方式，进行参数的估计。形式化通常有两种形式
，如下：&lt;/p&gt;
&lt;p&gt;$$ {\theta}^* = arg\max_{ {\theta}} p(\mathbf{t}|\mathbf{X}, {\theta}) =
arg\max_{ {\theta}} \prod_{i=1}^m p(t_i|\mathbf{x}_i, {\theta})
$$&lt;/p&gt;
&lt;p&gt;这种求解方法，称为最大似然估计(Maximum Likelihood Estimate, MLE)；或是
 $$
   {\theta}^* = arg\max_{ {\theta}} p( {\theta}|\mathbf{X},\mathbf{t}) =
arg\max_{ {\theta}} \prod_{i=1}^m p( {\theta}|\mathbf{x}_i,t_i)
$$&lt;/p&gt;
&lt;p&gt;这种求解方法，称为最大后验估计(Maximum Posterior Estimate, MAP)，其中， $p$ 为数据服从的分布。&lt;/p&gt;
&lt;p&gt;分类问题又称为模式识别(pattern recognition)，在第一节中所举的鸢尾花品种识别就是一个典型的分类问题，这里涉及到三个类别，每个实例只能属于其中
一个类别，按照我们前面的论述，该问题属于多类问题。其处理流程大致如图所示:&lt;/p&gt;
&lt;p&gt;&lt;img src="images/1-3.png", height=300pt, width=400pt/&gt;&lt;/p&gt;
&lt;p&gt;上面的流程也是大多数监督学习算法的流程，在其中最重要，也是最困难的的两个地方就是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;建模。选择合适的模型，设计好要优化的目标函数。有时候可能问题的定义导致不能显式的写出一个优化目标函数。&lt;/li&gt;
&lt;li&gt;学习。设计有效的学习算法进行模型的学习，是我们在学习现有的模型中遇到的困难的地方，大多数的学习算法都有很多细节需要处理，但是在讨论的时候，需要忽略掉一些细节
，进行抽象。&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;回归&lt;/h5&gt;
&lt;p&gt;回归问题也属于监督学习问题，与分类问题不同，在回归问题中目标值是连续的，目标值一般都是有意义的，比如在预测明天的气温，如果明天的实际气温是12度，那么预测的是1
5度与18 度差别是很大的。这与分类问题是有很大差别的，比如在字符识别中，要识别5，我们识别成6还是7，都是一样的结果，识别都是错误的。
回归问题的定义如下，对于数据集 $\mathbf{D}$ 中的每个实例，寻找一个函数 $h(\mathbf{x};
{\theta}):\mathbf{x}\in \mathbb{R}^n \rightarrow t\in \mathbb{R}$，其中$ {\theta}$
称为模型的参数。按照输入变量的个数，如果输入变量是一个变量，则成为一元回归，否则，输入变量有多个变量，称为多元回归。从概率角度理解，回归问题与分类问题类似，寻找
再给出 $\mathbf{x}, {\theta}$  的条件下 $t$  的分布 $p(t|\mathbf{x}, {\theta})$ ，两者主要的区别是
$t$ 服从的分布是连续分布，而不再是离散分布。&lt;/p&gt;
&lt;p&gt;曲线拟合中，需要用观测到的实值变量来预测实值的目标变量，是一个典型的回归问题。回归问题中常用的误差函数(error function)是平方误差(squared
loss)。误差函数描述训练集中模型的预测值与实际值的差距。如下:&lt;/p&gt;
&lt;p&gt;$$\mathbf J (\theta)=\frac{1}{2}\sum_{i=1}^{m}{y(\mathbf
x_{i},\theta)-t_{i}}^{2}$$&lt;/p&gt;
&lt;p&gt;求解的过程一般就是对函数求导，得到关于 $\theta$ 的线性函数，从而可以得到一个解析解 $ \theta^*$ 。详细的过程会在后面章节给出。&lt;/p&gt;
&lt;p&gt;在现实世界中，回归问题有很多应用：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在商业领域，可以用来预测预测股价，投资风险等；&lt;/li&gt;
&lt;li&gt;在互联网领域，可以预测网站的排名，用户的视频浏览量等；&lt;/li&gt;
&lt;li&gt;在机器人领域，可以用来预测机器人手臂的位置；&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;非监督学习&lt;/h4&gt;
&lt;p&gt;非监督学习问题与监督学习问题有很大的不同，主要体现在在非监督学习中目标变量没有给出，使得非监督学习不是良好定义(ill-
defined)问题。根据不同的问题，我们可以有不同的定义，从而有不同的模型。从概率角度来说，非监督学习主要是寻找数据的分布$p(x|
{\theta})$，其中$ {\theta}$为分布的参数。按照统计的术语来说，是密度估计。从概率方面来理解，主要有两个不同：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;监督学习是寻找给出 $x, {\theta}$ 的条件下 $t$  的分布 $p(t|x, {\theta})$
，求解的是一个条件分布；而非监督学习寻找数据的分布 $p(x| {\theta})$ ；&lt;/li&gt;
&lt;li&gt;通常来说，监督学习中的 $t$ 是单变量的，因此求解的是单变量的分布；而非监督学习中求解一个多变量的分布。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;非监督学习相比于监督学习来说，一个明显的优势是模型不需要人手动的给数据添加标记，现在的数据越来越大，标记数据是一个既耗时费力，又花费比较大。本书中会涉及到很多非
监督学习的内容，比如聚类，PCA，隐变量发现等。有人认为非监督学习更符合人学习的本质，著名的机器学习专家Hinton认为：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;当我们学习看这个世界时，我们并没有被告诉正确的结果，我们只是在看。即使你的母亲告诉“那是一条狗”，但是这本身的信息很少。按照这种方式，一秒钟得到1比特的信息已
经很幸运了。人脑的视觉系统有 $10^{14}$ 的神经连接，我们人类只有 $10^9$ 秒寿命。所以你 $1$ 秒学习 $1$ 比特的信息是无用的，至少要
$1$ 秒学习 $10^5$ 比特的信息。而且你只能从输入本身得到这么多的信息，从其他地方根本无法得到。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;根据上面的论述，可以用下面的框图3表示机器学习的分类：&lt;/p&gt;
&lt;p&gt;&lt;img src="images/1-2.png"/&gt;&lt;/p&gt;
&lt;p&gt;这只是一个简单的框架，它省略了很多机器学习的重要部分，比如按照分类器分类能力的强弱会有强弱分类器，可以集成多个弱分类器进行集成学习(ensemble
learning)，可以使用标记数据和没有标记数据的半间督学习，对于多标记的输出，我们可以考虑输出的结构(structure prediction)等。&lt;/p&gt;
&lt;h3&gt;数据&lt;/h3&gt;
&lt;p&gt;现在是数据爆炸的“大数据时代”，人类在各个领域积累了很多的数据，尤其是互联网上的数据，怎么从这些数据中挖掘有价值的东西？可以使用什么样的工具？这都是摆在面前的有
意义的问题。可以说，机器学习是用来进行数据分析，从中挖掘有意义和价值的模式的最好的工具。对数据的形式有一个大致的了解，是设计、应用机器学习的基础。对数据更好的认
识，有利于设计更好的学习算法。此外，如果对数据有了比较好的认识，可以借鉴处理相似数据的时候使用的模型，从而快速的进行问题的解决。&lt;/p&gt;
&lt;p&gt;如何采集数据，如何存储，不是本书的论述范围。讨论模型的时候，数据已经做了抽象，形成了高级的数据对象，这里只是简单的进行说明。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;向量(vector)：这是表示训练集实例最常用的方式，也是本书中描述实例最常用到的表示。将现实世界中的对象抽象出各种属性(可能不是显式的)，然后用向量进行表
示对象是最常用的方式，比如在描述文档时候，可以将其组织成一个词汇表中出现单词的向量，某个词汇出现，该位置为1，否则为0；&lt;/li&gt;
&lt;li&gt;列表(list)：列表与向量不同的是不同实例的可以有不同数目的特征；&lt;/li&gt;
&lt;li&gt;集合(set)：用集合来表示实例在多实例学习中是常用的(multi-instance learning)。通常是实例包含很多潜在的因素，而这些潜在因素的影响
并不明确。例如在确定分子在某种新药中的有效性时，由于同一个分子有很多变种，不能很准确的确定每个变种的有效性（正例还是负例），就将一个分子表示成一个集合（包），里
面包含不同的分子变种；&lt;/li&gt;
&lt;li&gt;矩阵(matrices)：用矩阵可以来表示这个对象之间(pairwise )的关系,比较常用的矩阵有：相似度矩阵，用在协同过滤中的用户-产品矩阵等；&lt;/li&gt;
&lt;li&gt;树或是图：实例构成树或是图的节点，然后依此来做推断；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;使用不同的数据表示，可能决定后面使用的机器学习算法，从而对问题解决有一个比较重要的影响。&lt;/p&gt;
&lt;h3&gt;参数化，非参数化模型&lt;/h3&gt;
&lt;p&gt;由前面的内容知道，从概率角度讲，机器学习求解$p(t|x)$（监督学习）或是$p(x)$（非监督学习）。定义这样的模型有很多方式，其中一种方法是：模型中含有的参
数是固定，还是不固定，随着数据集增长而增长。参数固定的模型称为参数化模型，而参数不固定的模型称为非参数化模型。参数化估计的优点是求解相对简单，缺点是对数据的分布
做了太强的假设，可能模型分布与数据分布之间的差别较大。非参数化模型相比参数化模型来说更灵活，但是对于大数据来说，模型求解却更加复杂。&lt;/p&gt;
&lt;h4&gt;参数化模型&lt;/h4&gt;
&lt;p&gt;前面提到的单变量多项式拟合就是一个比较简单的参数化模型，假设目标值与输入之间符合下面的式子，
 $$ t = \theta_{0}+ \theta_{1}x +  \theta_{2}x^2 +... + \theta_{n}x^n
+\epsilon$$&lt;/p&gt;
&lt;p&gt;其中， $\epsilon$ 称为残差(residual),假设符合符合高斯分布 $\mathcal N(\epsilon|0,\beta)$
。那么目标变量$t$的分布函数为：
 $$p(y|X,\theta,\beta)=\mathcal N(t|\theta_{0}+ \theta_{1}x +... +
\theta_{n}x^n,\beta)$$&lt;/p&gt;
&lt;p&gt;根据第二节中讨论的参数估计(MLE或是MAP)方式，我们可以求出 $ {\theta}$ 。&lt;/p&gt;
&lt;h4&gt;非参数化模型&lt;/h4&gt;
&lt;p&gt;在这里举一个K-近邻的方法来说明非参数话估计。简单的说，K-近邻方法就是用某个样本周围最邻近的K个样本的标记来决定该样本的标记。这种学习方式称为基于实例的学习
(instance-based learning)或是基于记忆的学习(memory-based learning)。概率分布可以形式化如下:&lt;/p&gt;
&lt;p&gt;$$p(t|\mathbf{x},D,K) = \frac{1}{K} \sum_{i\in NK(\mathbf{x})} \mathbb{I}
(\mathbf{x}_i==t)$$&lt;/p&gt;
&lt;p&gt;其中， $NK(x)$ 表示 $x$ 最近的 $K$ 个点的索引， $\mathbb{I}$
为&lt;a href="http://zh.wikipedia.org/zh/%E6%8C%87%E7%A4%BA%E5%87%BD%E6%95%B0"&gt;指示函数&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;此外，最近贝叶斯参数估计受到了越来越多的注意，后面的部分会对相关的内容进行讨论。&lt;/p&gt;
&lt;h3&gt;过拟合(Overfitting)&lt;/h3&gt;
&lt;p&gt;再来看一下多项式拟合的例子，图7中用20次多项式拟合21个数据点时候，函数穿过了所有的数据点，在训练集上的错误均值(mean square erro,MSE)为
0。当有新的实例到来时候，这样的函数预测性能很差，也就是模型的泛化能力不够。这种现象称为过拟合。这是由于训练数据集只能占到数据总体的一部分，在训练数据上完全拟合
，可能不能反映数据模型的真实分布，从而造成泛化能力很差。解决过拟合问题，提高模型的泛化能力，是机器学习的核心目标，通常的方法有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;降低特征数目；&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;人为的选择部分特征，忽略掉其他的特征；&lt;/li&gt;
&lt;li&gt;使用模型选择算法，进行特征选择。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;使用正则化(regularization)方法；&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;保留所有的特征，减小参数的数值；&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;模型选择&lt;/h3&gt;
&lt;p&gt;在第一节中已经说过，机器学习分为训练阶段和测试阶段，我们可以用某种标准来衡量模型，比如在回归问题中，用错误均值(MSE)。在训练阶段，我们可以得出一个训练错误(
training error )，但是我们更关注的是测试阶段的泛化错误(generalized error)。 从多项式拟合的例子可以看到,用1次多项式拟合的时
候，模型都不能很好的拟合训练数据集中的点，训练错误高，泛化错误高，这种现象称为欠拟合(underfits)。当用20次多项式拟合的时候，出现了过拟合的现象。在实
际的应用中，我们需要选择恰当的模型，使得模型能有比较好的泛化能力。&lt;/p&gt;
&lt;p&gt;但是在实际中，测试集是未知的，在模型训练和选择阶段是不可见的，只能用来评估模型的准确性。在实际中，通常会选择将训练集分为两部分：训练集(training
set)和验证集(validation set)，用训练集训练模型，然后使用验证集来验证模型，选在在验证集上表现最后的模型作为最终的模型，选定模型后，再在整个训
练集(包含验证集)上重新训练。通常，80%作为训练集，20%作为测试集。最后使用测试集对模型评估。&lt;/p&gt;
&lt;p&gt;实际应用中，数据集的规模是有限的，使用上面的方式会浪费很多的数据，验证集过小，给出的估计的噪声又很大，所以人们提出了一种简单有效的方式-交叉验证(cross
validation)用来进行模型的选择。交叉验证的方式很简单：将训练集数据分为K-折(fold)（常用的方式就是均分），其中K-1折的数据用来训练模型，剩下的
用来做验证集评估模型，重复进行K次，每一折都被当作过验证集，最后把K次的结果平均，用来作为这个模型的评估。如果K=m，则成为留一验证(leave-one-
out,LOOCV)。交叉验证有如下的缺点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;训练的次数随着K而增长，这对那些训练一个模型就需要很长时间的问题来说，就需要更多的时间&lt;/li&gt;
&lt;li&gt;如果模型中有多个参数需要选择，那么这些参数的组合可能导致训练次数呈指数增长；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;归根到底就是，在大数据情况下，模型训练本省就非常耗时，而交叉验证耗时更多，使得最终的程序没有良好的伸缩性(scalable)。&lt;/p&gt;
&lt;h3&gt;维度灾难(Curse of dimensionality)&lt;/h3&gt;
&lt;p&gt;我们这里看一个例子(在PRML和MLAPP里面都提到)。比如数据分布在 $D$ 维的单位超立方体中，现在我们要用KNN方法来进行分类，假设对某一个测试点 $x$
，要满足训练集中该点周围的点的比例要占到数据集的 $f$ 分之一。则我们可以以 $x$ 为中心，做一个超立方体，则该立方体的边长则至少要满足：
$e=f^{\frac{1}{D}}$ 。比如，要包含十分之一的数据，即 $f=10$ ，当 $D=1$  时， $e_1=0.1$ ，但是当 $D=10$
时， $e_{10}=0.79$
，这样数据就看起来不再是“局部的”，那么模型估计可能就不是很好。这种随着空间维度增加而出现的问题，称为维度灾难。下图表示了随着$e$变化$f$的变化情况。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;e&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;d_s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;d_s&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;d=&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;e&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Volumn fraction&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="images/a_1_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;从图中可以看出：即使在比例比较小的情况下，当维度很高的时候，超立方体的边长都很大，这样只有小部分数据点离测试点较近，大多数数据点都在离要测试的数据较远的一个狭长
区域。这也就说明在高维空间中，训练数据会变得比较稀疏，为了支持需要的结果，数据集可能需要指数型的增长，这样在低维空间中的直观想法，可能由于维度灾难的出现，而不能
直接在高维空间中直接的使用。&lt;/p&gt;
&lt;p&gt;在具体的机器学习问题中，虽然维度灾难有很大的影响，但是这并不妨碍我们寻找在高维空间中有效的方法，原因有二:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;高维空间中特征冗余，真正的数据存在于一个低纬度的空间，特别需要指出的是目标值的重要的变化方向可能很局限(PCA的原理)，这样可以采用一些降维的方法进行处理；&lt;/li&gt;
&lt;li&gt;真实的数据满足一些光滑的特性(可能是局部性)，这样输入值的变化就会导致目标值的相应变化，这样可以采用一些类似插值的技巧进行处理；&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;工具&lt;/h3&gt;
&lt;p&gt;本书以Python语言来实现算法，用到Scikit-learn软件包等第三方机器学习相关的软件。使用IPython
Notebook可以很容易的来进行学习。下面看几个简单的例子。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;sklearn.datasets&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;datasets&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pprint&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;pprint&lt;/span&gt;
&lt;span class="n"&gt;dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;datasets&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load_iris&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;The first 5 feature vectors in dataset:&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;pprint&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;The first 5 target values in dataset:&amp;#39;&lt;/span&gt; 
&lt;span class="n"&gt;pprint&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;下面看一下用Scikit-learn对上面提到的鸢尾花进行品种的分类，代码非常简洁。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;linear_model&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;lm&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.cross_validation&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.metrics&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;classification_report&lt;/span&gt;
&lt;span class="c"&gt;# split the iris dataset into training dataset and test dataset&lt;/span&gt;
&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
&lt;span class="c"&gt;# get the logistic regression classifier  &lt;/span&gt;
&lt;span class="n"&gt;lr_clf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LogisticRegression&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;lr_clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c"&gt;# predict the label for test dataset&lt;/span&gt;
&lt;span class="n"&gt;y_pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lr_clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c"&gt;# report the evalutation&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Detail classification report:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;classification_report&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;Detail&lt;/span&gt; &lt;span class="n"&gt;classification&lt;/span&gt; &lt;span class="n"&gt;report&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
             &lt;span class="n"&gt;precision&lt;/span&gt;    &lt;span class="n"&gt;recall&lt;/span&gt;  &lt;span class="n"&gt;f1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt;   &lt;span class="n"&gt;support&lt;/span&gt;

          &lt;span class="mi"&gt;0&lt;/span&gt;       &lt;span class="mf"&gt;1.00&lt;/span&gt;      &lt;span class="mf"&gt;1.00&lt;/span&gt;      &lt;span class="mf"&gt;1.00&lt;/span&gt;        &lt;span class="mi"&gt;11&lt;/span&gt;
          &lt;span class="mi"&gt;1&lt;/span&gt;       &lt;span class="mf"&gt;1.00&lt;/span&gt;      &lt;span class="mf"&gt;0.62&lt;/span&gt;      &lt;span class="mf"&gt;0.76&lt;/span&gt;        &lt;span class="mi"&gt;13&lt;/span&gt;
          &lt;span class="mi"&gt;2&lt;/span&gt;       &lt;span class="mf"&gt;0.55&lt;/span&gt;      &lt;span class="mf"&gt;1.00&lt;/span&gt;      &lt;span class="mf"&gt;0.71&lt;/span&gt;         &lt;span class="mi"&gt;6&lt;/span&gt;

&lt;span class="n"&gt;avg&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;total&lt;/span&gt;       &lt;span class="mf"&gt;0.91&lt;/span&gt;      &lt;span class="mf"&gt;0.83&lt;/span&gt;      &lt;span class="mf"&gt;0.84&lt;/span&gt;        &lt;span class="mi"&gt;30&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;下面看一下如何来进行模型选择，进行模型选择的方式有比较多，在Scikit-learn中cross_validation模块就有K-
flod，LOO，LPO等。下面的例子中，使用网格法来确定模型的超参数，使用最常用的K-flod来进行模型的选择。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;grid_search&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.metrics&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;classification_report&lt;/span&gt;
&lt;span class="n"&gt;parameters&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;C&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:[&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;]}&lt;/span&gt;
&lt;span class="n"&gt;clf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;grid_search&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GridSearchCV&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lr_clf&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Best parameters found on train set:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_estimator_&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mean_score&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scores&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grid_scores_&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="si"&gt;%0.3f&lt;/span&gt;&lt;span class="s"&gt; (+/- &lt;/span&gt;&lt;span class="si"&gt;%0.03f&lt;/span&gt;&lt;span class="s"&gt;) for &lt;/span&gt;&lt;span class="si"&gt;%r&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mean_score&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; 
&lt;span class="n"&gt;y_true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Detail classification report:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;classification_report&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;通过上面的例子可以看到，通过使用交叉验证我们得到了一个更好的模型，来对测试集中的数据进行预测。&lt;/p&gt;
&lt;h3&gt;引用&lt;/h3&gt;
&lt;p&gt;[1] 机器学习, Tom M. Mitchell, 曾华军等译，机械工业出版社, 2003.&lt;/p&gt;
&lt;p&gt;[2] Machine Learning: a Probabilistic Perspective, Kevin Patrick Murphy, 2012&lt;/p&gt;
&lt;p&gt;[3] Pattern Recognition and Machine Learning, Christopher M. Bishop, 2006&lt;/p&gt;
&lt;p&gt;[4] Scikit-learn: &lt;a href="http://scikit-
learn.org/stable/index.html"&gt;http://scikit-learn.org/stable/index.html&lt;/a&gt;&lt;/p&gt;
&lt;style&gt;
    @font-face {
        font-family: "Computer Modern";
        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunss.otf');
    }
    @font-face {
        font-family: "Computer Modern";
        font-weight: bold;
        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsx.otf');
    }
    @font-face {
        font-family: "Computer Modern";
        font-style: oblique;
        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsi.otf');
    }
    @font-face {
        font-family: "Computer Modern";
        font-weight: bold;
        font-style: oblique;
        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunso.otf');
    }
    div.cell{
        width:800px;
        margin-left:16% !important;
        margin-right:auto;
    }
    h1 {
        font-family: Helvetica, serif;
    }
    h4{
        margin-top:12px;
        margin-bottom: 3px;
       }
    h5{
        margin-top:11px;
        margin-bottom: 3px;
       }
    div.text_cell_render{
        font-family: Computer Modern, "Helvetica Neue", Arial, Helvetica, "Microsoft YaHei", Geneva, sans-serif;
        line-height: 145%;
        font-size: 108%;
        width:800px;
        margin-left:auto;
        margin-right:auto;
    }
    .CodeMirror{
            font-family: "Source Code Pro", source-code-pro,Consolas, monospace;
    }
    .prompt{
        display: None;
    }

    .rendered_html blockquote {
        margin: 1em 2em;
    }
    blockquote {
    padding: 0 0 0 15px;
    margin: 0 0 20px;
    border-left: 5px solid #eee;
    }
   blockquote {
    display: block;
    -webkit-margin-before: 1em;
    -webkit-margin-after: 1em;
    -webkit-margin-start: 40px;
    -webkit-margin-end: 40px;
    }
/*    .text_cell_render h5 {
        font-weight: 300;
        font-size: 10pt;
        color: #4057A1;
        font-style: italic;
        margin-bottom: .5em;
        margin-top: 0.5em;
        display: block;
    }*/

    .warning{
        color: rgb( 240, 20, 20 )
        }  
&lt;/style&gt;

&lt;script&gt;
    MathJax.Hub.Config({
                        TeX: {
                           extensions: ["AMSmath.js"]
                           },
                tex2jax: {
                    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
                },
                displayAlign: 'center', // Change this to 'center' to center equations.
                "HTML-CSS": {
                    styles: {'.MathJax_Display': {"margin": 4}}
                }
        });
&lt;/script&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</summary><category term="Machine Learning"></category></entry></feed>