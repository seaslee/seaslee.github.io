<!DOCTYPE html>
<html lang="en-US">
    <head>
        <meta charset="utf-8"> 
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="author" content="Webdancer" />
        <meta name="copyright" content="Webdancer" />

<meta name="keywords" content="Machine Learning, Machine Learning, " />
        <title>logistic Regression 模型 - AI's bazaar
</title>
        <link href="http://cdn-images.mailchimp.com/embedcode/slim-081711.css" rel="stylesheet" type="text/css">
        <link href="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.1/css/bootstrap-combined.min.css" rel="stylesheet">
        <link rel="stylesheet" type="text/css" href="blog.aisbazaar.com/theme/css/style.css" media="screen">
        <link rel="stylesheet" type="text/css" href="blog.aisbazaar.com/theme/css/solarizedlight.css" media="screen">
        <link rel="shortcut icon" href="blog.aisbazaar.com/theme/images/favicon.ico" type="image/x-icon" />
        <link rel="apple-touch-icon" href="blog.aisbazaar.com/theme/images/apple-touch-icon.png" />
        <link rel="apple-touch-icon" sizes="57x57" href="blog.aisbazaar.com/theme/images/apple-touch-icon-57x57.png" />
        <link rel="apple-touch-icon" sizes="72x72" href="blog.aisbazaar.com/theme/images/apple-touch-icon-72x72.png" />
        <link rel="apple-touch-icon" sizes="114x114" href="blog.aisbazaar.com/theme/images/apple-touch-icon-114x114.png" />
        <link rel="apple-touch-icon" sizes="144x144" href="blog.aisbazaar.com/theme/images/apple-touch-icon-144x144.png" />
        <link rel="icon" href="blog.aisbazaar.com/theme/images/apple-touch-icon-144x144.png" />
    </head>
    <body>
        <div id="content-sans-footer">
        <div class="navbar navbar-static-top">
            <div class="navbar-inner">
                <div class="container">
                    <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </a>
                    <a class="brand" href="blog.aisbazaar.com/"><span class=site-name>AI's bazaar</span></a>
                    <div class="nav-collapse collapse">
                        <ul class="nav pull-right top-menu">
                            <li ><a href="blog.aisbazaar.com">Home</a></li>
                            <li ><a href="blog.aisbazaar.com/categories.html">Categories</a></li>
                            <li ><a href="blog.aisbazaar.com/tags.html">Tags</a></li>
                            <li ><a href="blog.aisbazaar.com/archives.html">Archives</a></li>
                            <li><form class="navbar-search" action="blog.aisbazaar.com/search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
        <div class="container-fluid">
            <div class="row-fluid">
                <div class="span1"></div>
                <div class="span10">
<article>
<div class="row-fluid">
    <header class="page_header span10 offset2">
    <h1><a href="blog.aisbazaar.com/posts/2014/05/lr_model/"> logistic Regression 模型  </a></h1>
    </header>
</div>

<div class="row-fluid">
        <div class="span8 offset2 article-content">

            <p>前面已经学习了线性回归模型来解决回归问题，本章讨论解决分类问题的相关模型（也叫做分类器，classifier）。构建概率分类器有两种常用的方式：概率判别模型和概
率生成模型。概率判别模型是直接给出后验概率$p(t|\mathbf x)$；概率生成模型中，首先假设先验$p(t)$和似然函数$p(\mathbf
x|t)$，然后根据贝叶斯定理,计算出后验概率$p(t|\mathbf x)$。主要内容是讨论分类常用的线性模型logistic regression，
它是一种用来解决分类问题的概率判别模型。</p>
<h3>Logistic Regression Model</h3>
<p>在第一章中已经讨论过，最基本的分类问题是二类问题，即目标值中有两个类别，分类就是寻找一个决策面将这两类点分开。在LR （Logistic
Regression）模型中决策面是一个平面，让类别标签分别用$t_1=1,t_2=0$ 来表示，在LR模型中$t_1$ 的后验概率可以表示为：</p>
<p>\begin{equation}
p(t_1|\mathbf x,{\theta}) = \sigma({\theta}^{T}\mathbf x)
\label{labelpos}
\end{equation}</p>
<p>其中，$\mathbf x$为特征向量，${\theta}$为模型的参数，$\sigma(.)$为logistic sigmoid
function，其定义如下：</p>
<p>\begin{equation}
\sigma(z)=\frac{1}{1+e^{-z}}
\label{sigmoidfun}
\end{equation}</p>
<div class="highlight"><pre><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
<span class="n">sigmoid</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">&#39;sigmoid(x)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img alt="png" src="/images/d_1_0.png" /></p>
<p>根据概率的性质，那$t_2$的概率值为：</p>
<p>\begin{equation}
p(t_2|\mathbf x,{\theta}) = 1-p(t_1|\mathbf x,{\theta}) =  \sigma(-
{\theta}^{T}\mathbf x)
\label{labelpos2}
\end{equation}</p>
<p>可以看出，目标随机变量$t$服从伯努利分布，这是一种离散分布，在前面的模型中，因为目标值为连续的值，我们使用的是高斯分布，而在这里使用的是伯努利分布。</p>
<p>设已知训练集$\{\mathbf x_n,t_n\}$,其中$t_n \in
\{0,1\}$。我们假定$N$个观测变量符合独立同分布(i.i.d.)，则似然函数如下：</p>
<p>\begin{equation}
p(\mathbf t|\mathbf X,{\theta})=\prod_{n=1}^{N}{(y_n)}^{t_n}{(1-y_n)}^{(1-t_n)}
\label{mlf}
\end{equation}</p>
<p>其中，$\mathbf t=\{t_1,t_2,\cdots,t_N\}$,其中$y_n=p(t_1|\mathbf
x_n)$。从中可以看出，似然函数为二项分布,与线性回归的多变量高斯分布不同。$NLL$函数与前面的推导一致，可以写为对数似然的相反数,表示如下：</p>
<p>\begin{eqnarray}
\label{crossentro}
\mathbb E({\theta}) &amp;=&amp; -\ln p(\mathbf t|\mathbf X,{\theta})\\
\nonumber &amp;=&amp; -\ln[\prod_{n=1}^{N}{(y_n)}^{t_n}{(1-y_n)}^{(1-t_n)}]\\
\nonumber &amp;=&amp; -\sum_{n=1}^{N}[t_n\ln(y_n)+(1-t_n)\ln{(1-y_n)}]\\
\end{eqnarray}</p>
<p>上面得出的函数称为交叉熵误差函数(cross-entropy error function)。这样我们就得到了logistic
regression要优化的目标函数，下面要做的就是优化误差函数。通常有一种更经常使用的logistic loss function是$t_n \in
\{-1,+1\}$的情况，则似然函数如下：</p>
<p>\begin{equation}
p(\mathbf t|\mathbf X,{\theta})=\prod_{n=1}^{N} \sigma(t_ny_n)
\end{equation}</p>
<p>则可以得到logistic loss function如下：</p>
<p>\begin{eqnarray}
\mathbb E({\theta}) &amp;=&amp; -\ln p(\mathbf t|\mathbf X,{\theta})\\
\nonumber &amp;=&amp; -\ln\prod_{n=1}^{N} \sigma(t_ny_n)\\
\nonumber &amp;=&amp; -\sum_{n=1}^{N} \ln \sigma(t_ny_n)
\end{eqnarray}</p>
<h3>交叉熵误差函数优化</h3>
<p>由于sigmoid的加入的非线性，使得LR模型不再具有封闭解。但是交叉熵误差函数与平方和误差函数一样，都是一个<a href="http://en.wikipedia
.org/wiki/Convex_function">凸函数</a>。凸函数有一个很好的性质，至多有一个最小值，即局部极小值就是最小值。可以使用数值优化的方法求出极值点。</p>
<h4>梯度下降算法</h4>
<p>优化误差函数可以使用与第二章优化平方和误差函数的梯度下降算法（或是随机梯度下降）来求。在梯度下降算法中，梯度${\nabla} \mathbb
E$的计算最为关键，公式如下：</p>
<p>\begin{eqnarray}
{\nabla} \mathbb E({\theta}) &amp;=&amp;  \frac{\partial}{\partial {\theta}}\mathbb
E({\theta})\\
\nonumber &amp;=&amp; \sum_{n=1}^{N}(y_n-t_n)\mathbf x_n\\
\nonumber &amp;=&amp; \mathbf X^T(\mathbf y-\mathbf t)
\label{gradient}
\end{eqnarray}</p>
<p>则算法迭代公式如下：
\begin{equation}\label{gd}
  \theta =  \theta -\eta {\nabla} \mathbb E({\theta})
\end{equation}</p>
<p>对于随即梯度下降算法，迭代公式如下：
\begin{equation}\label{sgd}
  \theta =  \theta -\eta {\nabla} \mathbb E_n({\theta})
\end{equation}</p>
<p>由于使用梯度下降或是随机梯度下降与第三章中的线性回归模型是一致的，这里就不详细讨论。</p>
<h3>高级优化算法</h3>
<p>共轭梯度算法(conjugate gradient)，牛顿法(Newton
method)，L-BFGS算法等高级优化算法比梯度下降复杂，但是收敛的速度更快。在某些高级优化算法中，比如牛顿法，可能需要用到海森矩阵。计算公式如下：</p>
<p>\begin{eqnarray}
\mathbf H = {\nabla} {\nabla} \mathbb E({\theta}) &amp;=&amp;  \mathbf X^{T} \mathbf{S}
\mathbf X
\label{hessian}
\end{eqnarray}
其中，$ \mathbf{S}=diag(y_i(1-y_i)),i=1,2,...,N$。</p>
<p>使用牛顿法(Newton method)来进行优化，迭代公式如下：
\begin{equation}\label{newton}
  \theta=\theta - \mathbf{H}^{-1}{\nabla} \mathbb E({\theta})
\end{equation}</p>
<p>使用牛顿法解交叉熵误差函数，公式如下：
\begin{eqnarray}
% \nonumber to remove numbering (before each equation)
\theta &amp;= &amp;\theta - \mathbf{H}^{-1}{\nabla} \mathbb E({\theta})\\
\nonumber &amp;= &amp;\theta - ( \mathbf X^{T} \mathbf{S} \mathbf X)^{-1}\mathbf
X^T(\mathbf y-\mathbf t)\\
\nonumber &amp;=&amp; ( \mathbf X^{T} \mathbf{S} \mathbf X)^{-1}( \mathbf X^{T}
\mathbf{S} \mathbf X\theta-\mathbf X^T(\mathbf y-\mathbf t))\\
\nonumber &amp;=&amp; ( \mathbf X^{T} \mathbf{S} \mathbf X)^{-1}\mathbf X^{T} \mathbf{S}
( \mathbf X\theta-\mathbf{S}^{-1}(\mathbf y-\mathbf t))\\
\nonumber &amp;=&amp; ( \mathbf X^{T} \mathbf{S} \mathbf X)^{-1}\mathbf X^{T} \mathbf{S}
\mathbf{z}
\end{eqnarray}
该式子是加权的最小二乘的解，不过这里的$\mathbf{S}$不是常数，而是依赖于参数$\theta$，所以每次更新参数后，都需要重新使用该式子计算。所以该算法
也被称作 <strong>迭代加权最小二乘(iterative weighted least square,IRLS)</strong> ，其中$\mathbf{z}$ 是$m$
维的向量:</p>
<p>\begin{equation}\label{z}
  \mathbf{z} =\mathbf X\theta-\mathbf{S}^{-1}(\mathbf y-\mathbf t)
\end{equation}</p>
<h4>优化代码</h4>
<p>对于快速构建机器学习算法，可以使用现成的[优化类库][3]，尽量避免出错。在使用matlab的优化函数的时候，我们首先必须计算$\mathbb E({\theta}), {\nabla} \mathbb E({\theta})$,然后可以使用下面的代码来进行优化。</p>
<p>计算$\mathbb E({\theta}), {\nabla} \mathbb E({\theta})$ 来进行优化，代码格式如下所示：</p>
<div class="highlight"><pre><span class="nx">def</span> <span class="nx">costFunction</span><span class="p">(</span><span class="nx">X</span><span class="p">,</span><span class="nx">t</span><span class="p">,</span><span class="nx">theta</span><span class="p">)</span><span class="o">:</span>
    <span class="nx">cost_val</span> <span class="o">=</span> <span class="cp">[</span><span class="nx">...code</span> <span class="k">to</span> <span class="nx">compute</span> <span class="nx">E</span><span class="p">(</span><span class="nx">theta</span><span class="p">)</span><span class="nx">...</span><span class="cp">]</span>
    <span class="nx">gradient</span> <span class="o">=</span> <span class="cp">[</span><span class="nx">...code</span> <span class="k">to</span> <span class="nx">compute</span> <span class="nx">derivative</span> <span class="nx">of</span> <span class="nx">E</span><span class="p">(</span><span class="nx">theta</span><span class="p">)</span><span class="nx">...</span><span class="cp">]</span>
    <span class="k">return</span> <span class="nx">cost_val</span><span class="err">，</span><span class="nx">gradient</span>

<span class="nx">options</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;maxiter&#39;</span><span class="o">:</span><span class="nx">maxiter</span><span class="p">,</span> <span class="s1">&#39;disp&#39;</span><span class="o">:</span><span class="nx">False</span><span class="p">}</span>
<span class="nx">costFunction</span> <span class="o">=</span> <span class="nx">lambda</span> <span class="nx">theta</span><span class="o">:</span> <span class="nx">costFunction</span><span class="p">(</span><span class="nx">X</span><span class="p">,</span><span class="nx">t</span><span class="p">,</span><span class="nx">theta</span><span class="p">)</span>
<span class="nx">initialTheta</span> <span class="o">=</span> <span class="nx">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">);</span>
<span class="cp">[</span><span class="nx">optTheta</span><span class="p">,</span> <span class="nx">functionVal</span><span class="p">,</span> <span class="nx">exitFlag</span><span class="cp">]</span> <span class="o">=</span> <span class="nx">minimize</span><span class="p">(</span><span class="nx">costFunction</span><span class="p">,</span>
                                            <span class="nx">initialTheta</span><span class="p">,</span>
                                            <span class="nx">method</span><span class="o">=</span><span class="s1">&#39;BFGS&#39;</span>
                                            <span class="nx">options</span><span class="p">);</span>
</pre></div>


<h3>多类LR模型</h3>
<p>基于以上讨论的二分类的logistic regression model，可以将LR模型进行扩展，用来处理多类问题。将logistic sigmoid
function 变为softmax function，LR模型就可以处理都分类问题了，这个过程非常容易（有很多模型，将其扩展到多类问题是比较困难的，比如后面要提到的SVM）。在多类时候，目标随机变量符合类别分布(categorical distribution),这样似然函数就可以写成下面的形式：</p>
<p>\begin{equation}\label{likelihood}
 p(\mathbf T|\mathbf X, \Theta) =
\prod_{n=1}^{N}\prod_{k=1}^{K}(y_{n,k})^{t_{n,k}}
\end{equation}</p>
<p>其中，$y_{n,k}=\frac{\exp(\theta_k \mathbf X_n)}{\sum_i \exp(\theta_i \mathbf
X_n)}$，$\Theta=[\theta_1,\theta_2,...,\theta_K]$，$\mathbf T$是$N\times
K$的矩阵，元素用$t_{n,k}$表示。其中，softmax function $soft:\mathbb{R}^N \rightarrow R$的形式如下：</p>
<p>\begin{equation}\label{softmax}
  soft(\mathbf x,k) = \frac{\exp(\mathbf x_k)}{\sum_j \exp(\mathbf x_j)}
\end{equation}</p>
<p>其中：
\begin{equation}\label{softmax-derivate}
 \frac{\partial y_k}{\partial \mathbf x_j} = y_k(\delta_{k,j}-y_j)
\end{equation}</p>
<p>\begin{equation}\label{delta}
 \delta_{k,j}=\left\{
  \begin{array}{ll}
    1,  \hbox{k=j;} \\
    0,  \hbox{otherwise.}
  \end{array}
\right.
 \end{equation}</p>
<p>这样我们很容易写出NLL 函数, 也就是我们的错误函数：</p>
<p>\begin{eqnarray}
\label{crossentro1}
\mathbb E( \Theta) &amp;=&amp; -\ln p(\mathbf T|\mathbf X,  \Theta)\\
\nonumber &amp;=&amp;- \sum_{n=1}^{N}\sum_{k=1}^{K}t_{n,k}\ln(y_{n,k})
\end{eqnarray}</p>
<p>这就是多类logistic regression要优化的目标函数。下面的目标就是优化该目标函数了，如果使用梯度下降方法，还得求梯度；如果使用其他的高级优化算法，
可能还需要求海森矩阵。梯度的求解过程如下：</p>
<p>\begin{eqnarray}
\label{gra2}
\frac{\partial \mathbb E(\Theta) }{\partial \theta_j} &amp;=&amp;\frac{\partial \mathbb
E(\Theta) }{\partial a_j}\frac{\partial a_j}{\partial \theta_j}\\
\nonumber &amp;=&amp; \sum_{n=1}^{N}\sum_{k=1}^{K}t_{n,k}\frac{1}{y_{n,k}}y_{n,k}(y_{n,j
}-I_{k,j})\mathbf x_n\\
\nonumber &amp;=&amp; \sum_{n=1}^{N}\sum_{k=1}^{K}t_{n,k}(y_{n,j}-I_{k,j})\mathbf
x_n\\
\nonumber &amp;=&amp;  \sum_{n=1}^{N}(y_{n,j}-t_{n,j})\mathbf x_n
\end{eqnarray}</p>
<p>所以有：</p>
<p>\begin{equation}\label{grad-multi-lr}
  \frac{\partial \mathbb E(\Theta) }{\partial \Theta} = \mathbf
X^T(\mathbf{T}-\mathbf{Y})
\end{equation}</p>
<p>其中，用到了$\frac{\partial y_k}{\partial a_j} = y_k(I_{k,j}-y_j)$，$ \sum_kt_{n,k}=1$。</p>
<p>海森矩阵的包含$K\times K$个大小为$D \times D$的子方阵，其中第$i,j$块子方阵求解公式如下</p>
<p>\begin{equation}
  \nabla_{\theta_j}\nabla_{\theta_k} =
\sum_{i=1}^{N}y_{n,j}(\delta_{j,k}-y_{n,k})\mathbf x\mathbf x^T
\end{equation}</p>
<p>由于海森矩阵的存储空间的复杂度为$O(D^2M^2)$，在处理高维数据和大数据时并不实用，例如一个商业的CRT预测系统的维度有几亿维，训练实例可能也有几亿个，则
需要的存储可能为$8\times 10^{32}$字节，需要的存储太大。</p>
<h3>正则化LR模型</h3>
<p>如同在概率一章和线性回归模型中讨论的最大似然估计容易过拟合，尤其是在小样本的情况下。如果在正则化线性回归模型一样，可以通过正则化框架（regularization framework）给LR模型添加一个正则化项，比如常用的$\ell_1$,$\ell_2$范数等，增强模型的泛化能力。如果使用$\ell_2$范数，对二分
类问题来说，目标函数可以写成下面的形式：</p>
<p>\begin{equation}\label{obj}
  J({\theta}) = arg\min_{\theta}\mathbb E({\theta})+\lambda \parallel {\theta}
\parallel_2 ^2
\end{equation}</p>
<p>对多类问题，则目标函数可以写成下面形式：</p>
<p>\begin{equation}\label{obj-multi}
  J(\Theta) = arg\min_{\Theta}\mathbb E(\Theta)+\sum_k^K\lambda_k \parallel
\theta_k \parallel_2 ^2
\end{equation}</p>
<p>求解上面的目标函数可以使用梯度下降等上面讨论的优化算法。</p>
<div class="highlight"><pre><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">minimize</span> 
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="c">#igmoid = lambda x: 1 / (1 + np.exp(-x))</span>

<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
    <span class="n">y</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]))</span>
    <span class="n">y_t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="o">~</span><span class="n">idx</span><span class="p">])</span> 
    <span class="n">y</span><span class="p">[</span><span class="o">~</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_t</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">+</span> <span class="n">y_t</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y</span>

<span class="k">class</span> <span class="nc">LogisticRegression</span><span class="p">:</span>
    <span class="sd">&#39;&#39;&#39; implement a logistic regression for </span>
<span class="sd">     binary classification&#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">maxiter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s">&#39;l2&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__tol</span> <span class="o">=</span> <span class="n">tol</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__maxiter</span> <span class="o">=</span> <span class="n">maxiter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__penalty</span> <span class="o">=</span> <span class="n">penalty</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__C</span> <span class="o">=</span> <span class="n">C</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__type</span> <span class="o">=</span> <span class="mi">0</span> <span class="c">#1: binary 2: multi</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">__cross_entropy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
        <span class="n">yz</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="n">z</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">yz</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="n">err</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">yz</span><span class="p">)</span>
        <span class="n">err</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log1p</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">yz</span><span class="p">[</span><span class="n">idx</span><span class="p">]))</span>
        <span class="n">err</span><span class="p">[</span><span class="o">~</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="n">yz</span><span class="p">[</span><span class="o">~</span><span class="n">idx</span><span class="p">]</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">log1p</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">yz</span><span class="p">[</span><span class="o">~</span><span class="n">idx</span><span class="p">]))</span>
        <span class="n">cost</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">err</span><span class="p">)</span> <span class="o">+</span> <span class="n">theta</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mf">2.</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">__C</span><span class="p">)</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">y</span><span class="o">*</span><span class="p">(</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">yz</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">+</span> <span class="n">theta</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">__C</span>
        <span class="k">return</span> <span class="n">cost</span><span class="p">,</span> <span class="n">grad</span>

    <span class="k">def</span> <span class="nf">__train_binary_logit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">n</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
        <span class="k">if</span> <span class="n">n</span><span class="o">!=</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s">&#39;The size of features and targets DON</span><span class="se">\&#39;</span><span class="s">T mathch&#39;</span><span class="p">)</span>
        <span class="c"># format data</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span> <span class="n">X</span><span class="p">))</span>
        <span class="c">#y = y.reshape(n,1) </span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">d</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
        <span class="c"># eta = .1</span>
        <span class="c"># while True:</span>
        <span class="c">#   err, grad = self.__cross_entropy(X,y,theta)</span>
        <span class="c">#   theta = theta - eta * grad</span>
        <span class="c">#   err1, grad1 = self.__cross_entropy(X, y, theta)</span>
        <span class="c">#   print &#39;the err&#39;, err</span>
        <span class="c">#   if abs(err1-err) &lt; self.__tol:</span>
        <span class="c">#       break</span>
        <span class="c"># self.weight = theta</span>
        <span class="n">options</span> <span class="o">=</span> <span class="p">{</span><span class="s">&#39;maxiter&#39;</span><span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">__maxiter</span><span class="p">,</span> <span class="s">&#39;disp&#39;</span><span class="p">:</span><span class="bp">True</span><span class="p">}</span>
        <span class="n">loss_fun</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">theta</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">__cross_entropy</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">loss_fun</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s">&quot;BFGS&quot;</span><span class="p">,</span> <span class="n">jac</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">options</span><span class="o">=</span><span class="n">options</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">res</span><span class="o">.</span><span class="n">x</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39; make sure the data format here</span>
<span class="sd">         - X: n * d matrix</span>
<span class="sd">         - y: n array&#39;&#39;&#39;</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">c</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">__type</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">__train_binary_logit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">c</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">__type</span> <span class="o">=</span> <span class="mi">2</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">__train_multi_logit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">n</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span> <span class="n">X</span><span class="p">))</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">__type</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">p</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">))</span>
            <span class="n">p</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">y_pred</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="n">y_pred</span><span class="p">[</span><span class="n">p</span><span class="o">&lt;</span><span class="mf">0.5</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">y_pred</span>


<span class="k">if</span> <span class="n">__name__</span><span class="o">==</span><span class="s">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="c"># dataset</span>
    <span class="n">datasets</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">data</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">target</span>
    <span class="n">ind</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logical_or</span><span class="p">(</span><span class="n">y</span><span class="o">==</span><span class="mi">0</span><span class="p">,</span><span class="n">y</span><span class="o">==</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">ind</span><span class="p">,</span><span class="mi">1</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span>
    <span class="n">y</span><span class="p">[</span><span class="n">y</span><span class="o">==</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="c">#plt.show()</span>
    <span class="n">clf</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">print</span> <span class="s">&#39;the training acc is &#39;</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y_pred</span><span class="o">==</span><span class="n">y</span><span class="p">))</span> <span class="o">/</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">y1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">X1</span><span class="p">,</span> <span class="n">Y1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">y1</span><span class="p">)</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">weight</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">X1</span> <span class="o">+</span> <span class="n">w</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">*</span><span class="n">Y1</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">&#39;o&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span><span class="n">Y1</span><span class="p">,</span><span class="n">Z</span><span class="p">,[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">Optimization</span> <span class="n">terminated</span> <span class="n">successfully</span><span class="o">.</span>
         <span class="n">Current</span> <span class="n">function</span> <span class="n">value</span><span class="p">:</span> <span class="mf">0.000002</span>
         <span class="n">Iterations</span><span class="p">:</span> <span class="mi">29</span>
         <span class="n">Function</span> <span class="n">evaluations</span><span class="p">:</span> <span class="mi">35</span>
         <span class="n">Gradient</span> <span class="n">evaluations</span><span class="p">:</span> <span class="mi">35</span>
<span class="n">the</span> <span class="n">training</span> <span class="n">acc</span> <span class="ow">is</span>  <span class="mf">1.0</span>
</pre></div>


<p><img alt="png" src="/images/d_5_1.png" /></p>
<h3>引用</h3>
<p>[1] Grinstead and Snell's Introduction to Probability, Peter G. Doyle, 2006.</p>
<p>[2] Machine Learning: a Probabilistic Perspective, Kevin Patrick Murphy.</p>
<p>[3] 在coursera的机器学习课程，Andrew Ng教授建议新手使用现成的优化函数，因为数值优化可能存在大量的技巧。</p><script type= "text/javascript">
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
</script>

            <aside>
            <nav>
            <ul class="articles_timeline">
 
                <li class="next_article"><a href="blog.aisbazaar.com/posts/2014/05/linear_model/" title="Next: 线性回归模型">线性回归模型</a> »</li>
            </ul>
            </nav>
            </aside>
<section>
<div class="accordion" id="accordion2">
    <div class="accordion-group">
        <div class="accordion-heading">
            <a class="accordion-toggle disqus-comment-count" data-toggle="collapse" data-parent="#accordion2"
                href="blog.aisbazaar.com/posts/2014/05/lr_model//#disqus_thread">
                Comments
            </a>
        </div>
        <div id="disqus_thread" class="accordion-body collapse">
            <div class="accordion-inner">
                <div class="comments">
<div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'wbdai';
        var disqus_identifier = 'blog.aisbazaar.com/posts/2014/05/lr_model/';
    var disqus_url = 'blog.aisbazaar.com/posts/2014/05/lr_model/';

    (function() {
         var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
         dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
         (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
     })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>                </div>
            </div>
        </div>
    </div>
</div>
</section>
        </div>
        <section>
        <div class="span2" style="float:right;font-size:0.9em;">
 
            <h4>Published</h4>
            <time pubdate="pubdate" datetime="2014-05-02T21:00:00">May 2, 2014</time>
            <h4>Category</h4>
            <a class="category-link" href="/categories.html#Machine-Learning-ref">Machine Learning</a> 
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article"> 
                <li><a href="/tags.html#Machine-Learning-ref">Machine Learning
                    <span>4</span>
</a></li>
            </ul>

        </div>
        </section>
    </div>
    </article>
                </div>
                <div class="span1"></div>
            </div>
        </div>
    </div>
<footer>
<div id="footer">
    <ul class="footer-content">
        <li class="elegant-power">Copyright ©<a href=http://mima.sdu.edu.cn/Members/~xuxinchao/> Webdancer</a> 2014 - Powered by <a href="http://getpelican.com/" title="Pelican Home Page">Pelican</a>. Theme: <a href="http://oncrashreboot.com/pelican-elegant" title="Theme Elegant Home Page">Elegant</a> by <a href="http://oncrashreboot.com" title="Talha Mansoor Home Page">Talha Mansoor</a></li>
    </ul>
</div>
</footer>            <script src="http://code.jquery.com/jquery.min.js"></script>
        <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.1/js/bootstrap.min.js"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>

<script type="text/javascript">
    var disqus_shortname = 'wbdai';

    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = '//' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
    </script>
    </body>
</html>